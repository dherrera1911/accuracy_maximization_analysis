{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c720872c",
   "metadata": {},
   "source": [
    "# Training the AMA model\n",
    "\n",
    "In the previous tutorial we showed the AMA functionalities that\n",
    "allow to estimate the value of a latent variable from a stimulus.\n",
    "In this tutorial we show how to train the model to learn the optimal\n",
    "filters for the task.\n",
    "\n",
    "First we load the data and initialize the AMA model. See the\n",
    "previous tutorial for details on the data and the model initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfb0bd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### DOWNLOAD DISPARITY DATA\n",
    "%%capture\n",
    "!mkdir data\n",
    "!wget -O ./data/dspCtg.csv https://raw.githubusercontent.com/dherrera1911/accuracy_maximization_analysis/master/data/dspCtg.csv\n",
    "!wget -O ./data/dspStim.csv https://raw.githubusercontent.com/dherrera1911/accuracy_maximization_analysis/master/data/dspStim.csv\n",
    "!wget --no-check-certificate -O  ./data/dspVal.csv https://raw.githubusercontent.com/dherrera1911/accuracy_maximization_analysis/master/data/dspVal.csv\n",
    "!wget --no-check-certificate -O  ./data/dspFilters.csv https://raw.githubusercontent.com/dherrera1911/accuracy_maximization_analysis/master/data/dspFilters.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f051c4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FIRST WE NEED TO DOWNLOAD AND INSTALL GEOTORCH AND QUADRATIC RATIOS PACKAGES\n",
    "%%capture\n",
    "!pip install geotorch\n",
    "import geotorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6eccd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INSTALL THE AMA_LIBRARY PACKAGE FROM GITHUB\n",
    "%%capture\n",
    "!pip install git+https://github.com/dherrera1911/accuracy_maximization_analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d25193",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### IMPORT PACKAGES\n",
    "##############\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import ama_library.ama_class as cl\n",
    "import ama_library.utilities as au\n",
    "import ama_library.plotting as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867dfc7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### LOAD DISPARITY DATA\n",
    "##############\n",
    "# Load data from csv files\n",
    "# Load stimuli\n",
    "s = torch.tensor(np.loadtxt('./data/dspStim.csv', delimiter=','))\n",
    "s = s.transpose(0,1)\n",
    "s = s.float()\n",
    "nPixels = int(s.shape[1]/2)\n",
    "# Load the category of each stimulus\n",
    "ctgInd = np.loadtxt('./data/dspCtg.csv', delimiter=',')\n",
    "ctgInd = torch.tensor(ctgInd, dtype=torch.int64) - 1\n",
    "# Load the latent variable values\n",
    "ctgVal = torch.tensor(np.loadtxt('./data/dspVal.csv', delimiter=','))\n",
    "ctgVal = ctgVal.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4111c5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# INITIALIZE AMA MODEL\n",
    "##############\n",
    "# Set the parameters\n",
    "nFilt = 2  # Create the model with 2 filters\n",
    "pixelNoiseVar = 0.003  # Input pixel noise variance\n",
    "respNoiseVar = 0.005  # Filter response noise variance\n",
    "# Create the untrained AMA object\n",
    "ama = cl.AMA_emp(sAll=s, ctgInd=ctgInd, nFilt=nFilt,\n",
    "        respNoiseVar=respNoiseVar, pixelCov=pixelNoiseVar, ctgVal=ctgVal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19ea20",
   "metadata": {},
   "source": [
    "The model is initialized with random filters. Let's visualize the\n",
    "random filters.\n",
    "the optimal filters that we put into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88168fbe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define function to plot binocular stimulus\n",
    "def plot_binocular(bino):\n",
    "    nPixels = int(bino.shape[0]/2)\n",
    "    x = np.linspace(-30, 30, nPixels)\n",
    "    # Plot the binocular 1D images\n",
    "    plt.rcParams.update({'font.size': 14})  # increase default font size\n",
    "    arcMin = np.linspace(start=-30, stop=30, num=nPixels) # x axis values\n",
    "    # Plot the binocular 1D images\n",
    "    plt.plot(x, bino[:nPixels], label='Left eye', color='red')  # plot left eye\n",
    "    plt.plot(x, bino[nPixels:], label='Right eye', color='blue')  #plot right eye\n",
    "    plt.xlabel('Visual field (arcmin)')\n",
    "\n",
    "\n",
    "##############\n",
    "# VISUALIZE MODEL FILTERS\n",
    "##############\n",
    "fPlot = ama.f.detach().clone()\n",
    "plt.subplot(1,2,1)\n",
    "plot_binocular(fPlot[0,:])\n",
    "plt.ylabel('Weight')\n",
    "plt.title(f'Filter 1')\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.subplot(1,2,2)\n",
    "plot_binocular(fPlot[1,:])\n",
    "plt.title(f'Filter 2')\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(11,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27111324",
   "metadata": {},
   "source": [
    "In the previous tutorial we showed that the learned filters separated\n",
    "the classes response distributions. However, the untrained filters\n",
    "are not expected to do this. Lets visualize the response distributions\n",
    "for two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ba4ed",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resp = ama.get_responses(s=s).detach()\n",
    "\n",
    "indPlot1 = 5 # Index of the first class to plot (use same as stim plotted above)\n",
    "indPlot2 = 12  # Index of the second class to plot\n",
    "respClass1 = resp[ctgInd==indPlot1, :]  # Get responses of class 1\n",
    "respClass2 = resp[ctgInd==indPlot2, :]  # Get responses of class 2\n",
    "plt.scatter(respClass1[:,0], respClass1[:,1], label=f'{ctgVal[indPlot1]} arcmin',\n",
    "            color='green', alpha=0.5)\n",
    "plt.scatter(respClass2[:,0], respClass2[:,1], label=f'{ctgVal[indPlot2]} arcmin',\n",
    "            color='brown', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Filter 1 response')\n",
    "plt.ylabel('Filter 2 response')\n",
    "plt.title('Untrained filter response to two classes')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(6,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccb5911",
   "metadata": {},
   "source": [
    "To learn the filters that maximize performance at the task we perform\n",
    "gradient descent, using the Pytorch tools for automatic differentiation.\n",
    "We can use different loss functions. In this case we will use a\n",
    "cross-entropy loss, that is, the negative log-posterior of the correct\n",
    "class:\n",
    "\n",
    "\\begin{equation}\n",
    "    L(R) = -\\log P(X=X_j | \\mathbf{R})\n",
    "\\end{equation}\n",
    "\n",
    "Let's show how to compute the loss and take a step in the gradient\n",
    "direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db854037",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize Pytorch optimizator for ama parameters\n",
    "learningRate = 1\n",
    "opt = torch.optim.Adam(ama.parameters(), lr=learningRate)\n",
    "opt.zero_grad() # Make sure gradients are zeroed\n",
    "# Compute the loss for each stimulus\n",
    "posteriors = ama.get_posteriors(s) # Get posteriors\n",
    "nStim = s.shape[0]\n",
    "loss = -torch.log(posteriors[torch.arange(nStim), ctgInd]) # select the correct class\n",
    "# Print the loss, see that the gradient is kept\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125e256",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now take the mean of the losses, to have a unique loss value\n",
    "lossMean = loss.mean()\n",
    "print(f'Initial loss: {lossMean.detach()}')\n",
    "# Compute the gradient of the loss with respect to the model parameters\n",
    "lossMean.backward()\n",
    "# Take a step in the gradient direction\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd328e",
   "metadata": {},
   "source": [
    "Now, remember that a key part of the AMA model is that we use the\n",
    "filter response statistics to decode the latent variable. Since we\n",
    "modified the filters with gradient descent we need to recompute the\n",
    "response statistics. This is done with the function\n",
    "`ama.update_response_statistics()`. This function makes use of the\n",
    "pre-processed stimulus statistics, that don't change with the filters\n",
    "(since pre-processing doesn't depend on the filters), and that are\n",
    "stored in the attributes `ama.stimMean` and `ama.stimCov`. Basic\n",
    "probability theory shows that the mean and covariance of a\n",
    "linearly transformed random variable $Y = f^TX$ (i.e. filter outputs) \n",
    "are given by $\\mu_{Y} = f^T \\mu_X$ and $\\Sigma_Y = f^T \\Sigma_X f$.\n",
    "\n",
    "When we create the AMA object, the pre-processed stimulus statistics\n",
    "are computed and stored. For the version of AMA implemented here\n",
    "(i.e. the empirical version), the pre-processed stimulus statistics\n",
    "are computed by taking each stimulus, applying the pre-processing\n",
    "(with many samples, since pre-processing is stochastic), and then\n",
    "taking the mean and covariance of the pre-processed stimuli.\n",
    "Thus, we do not need to pass the stimuli again for computing the\n",
    "response statistics.\n",
    "\n",
    "We update the response statistics, and then compute the loss again\n",
    "to see if it decreased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73663623",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ama.update_response_statistics()\n",
    "# Compute the loss for each stimulus\n",
    "posteriors = ama.get_posteriors(s) # Get posteriors\n",
    "nStim = s.shape[0]\n",
    "loss = -torch.log(posteriors[torch.arange(nStim), ctgInd]) # select the correct class\n",
    "lossMean = loss.mean()  # Take the mean of the losses\n",
    "# Print the loss, see that the gradient is kept\n",
    "print(f'Loss after step: {lossMean.detach()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb95a82",
   "metadata": {},
   "source": [
    "We see that the loss decreased as expected. The utility functions in the\n",
    "AMA package include a basic fitting function that performs the\n",
    "gradient descent procedure for a number of epochs, including the\n",
    "statistics updating. We will use this function to train the model.\n",
    "We also use the loss functions included in the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82375846",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nEpochs = 30\n",
    "learningRate = 0.05\n",
    "lrStepSize = 5 # number of epochs between each lr decay\n",
    "lrGamma = 0.7   # multiplication factor for lr decay\n",
    "batchSize = 256\n",
    "\n",
    "# Put data into Torch data loader tools\n",
    "trainDataset = TensorDataset(s, ctgInd)\n",
    "# Batch loading and other utilities \n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
    "# Optimizer and scheduler\n",
    "opt = torch.optim.Adam(ama.parameters(), lr=learningRate)\n",
    "sch = torch.optim.lr_scheduler.StepLR(opt, step_size=lrStepSize, gamma=lrGamma)\n",
    "\n",
    "# Fit model\n",
    "loss, _, elapsedTimes = au.fit(nEpochs=nEpochs, model=ama,\n",
    "                            trainDataLoader=trainDataLoader,\n",
    "                            lossFun=au.cross_entropy_loss, opt=opt,\n",
    "                            scheduler=sch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac15d4",
   "metadata": {},
   "source": [
    "Let's visualize the new filters and how the loss function changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa32dc8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# VISUALIZE MODEL FILTERS\n",
    "##############\n",
    "fPlot = ama.f.detach().clone()\n",
    "plt.subplot(1,2,1)\n",
    "plot_binocular(fPlot[0,:])\n",
    "plt.ylabel('Weight')\n",
    "plt.title(f'Filter 1')\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.subplot(1,2,2)\n",
    "plot_binocular(fPlot[1,:])\n",
    "plt.title(f'Filter 2')\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(11,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d7b69",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# VISUALIZE LOSS\n",
    "##############\n",
    "plt.plot(torch.arange(nEpochs+1), loss)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8858200",
   "metadata": {},
   "source": [
    "Now, lets compare the response distributions of the trained model\n",
    "with the untrained model for the same two classes as before.\n",
    "We expect that the response distributions are more separated after\n",
    "training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644970f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resp = ama.get_responses(s=s).detach()\n",
    "\n",
    "indPlot1 = 5 # Index of the first class to plot (use same as stim plotted above)\n",
    "indPlot2 = 12  # Index of the second class to plot\n",
    "respClass1 = resp[ctgInd==indPlot1, :]  # Get responses of class 1\n",
    "respClass2 = resp[ctgInd==indPlot2, :]  # Get responses of class 2\n",
    "plt.scatter(respClass1[:,0], respClass1[:,1], label=f'{ctgVal[indPlot1]} arcmin',\n",
    "            color='green', alpha=0.5)\n",
    "plt.scatter(respClass2[:,0], respClass2[:,1], label=f'{ctgVal[indPlot2]} arcmin',\n",
    "            color='brown', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Filter 1 response')\n",
    "plt.ylabel('Filter 2 response')\n",
    "plt.title('Trained filter response to two classes')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(6,6)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
