{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0347f03",
   "metadata": {},
   "source": [
    "# Approximating the covariance of noisy, normalized, neural responses\n",
    "\n",
    "Neuronal responses are noisy (i.e. there is variability in the\n",
    "responses to a repeated stimulus). Also, neuronal responses\n",
    "are usually divisively normalized by the pooled responses\n",
    "of other neurons. Thus, when modeling neuronal responses, it\n",
    "is convenient to apply these two pre-processing steps: additive\n",
    "noise and normalization.\n",
    "\n",
    "In the simplest case, noise is modeled as an isotropic Gaussian\n",
    "around the mean response, and normalization is broadband\n",
    "(i.e. the population response vector is divided by its norm).\n",
    "This is expressed in the following formula:\n",
    "\n",
    "$$c = \\frac{s + \\gamma}{||s + \\gamma||}$$\n",
    "\n",
    "where $s \\in \\mathbb{R}^d$ is the expected value of the\n",
    "population response vector, with $d$ dimensions,\n",
    "$\\gamma \\sim \\mathcal{N}(0,\\mathbf{I}\\sigma^2), \\gamma \\in \\mathbb{R}^d$\n",
    "is a sample of multivariate white noise, and $c \\in \\mathbb{R}^d$ is the\n",
    "noisy, normalized, population response.\n",
    "\n",
    "When studying the statistics of responses of model neurons\n",
    "to natural signals (e.g. for building probabilistic response\n",
    "models), it is thus desirable to compute the statistics\n",
    "of the noisy, normalized responses.\n",
    "\n",
    "Here, we present analytic formulas to compute the covariances\n",
    "of noisy, normalized responses, from the noiseless \n",
    "unnormalized responses. We first present the formula\n",
    "to obtain the second moment of the noisy normalized responses\n",
    "for single stimuli. Then, we use these to compute\n",
    "the covariance of the noisy normalized responses to a\n",
    "set of natural images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ae5ae9",
   "metadata": {},
   "source": [
    "# Isotropic Gaussian noise and broadband normalization\n",
    "\n",
    "In the case of $c = \\frac{s + \\gamma}{||s + \\gamma||}$\n",
    "with $\\gamma \\sim \\mathcal{N}(0, \\mathbf{I}\\sigma^2)$, there\n",
    "is an exact formula to compute the second moment\n",
    "$\\mathbb{E}_{\\gamma}(cc^T)$\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbb{E}_{\\gamma}\\left[cc^T \\right] =\n",
    "  \\frac{1}{d} {}_{1}F_1\\left(1; \\frac{d}{2}+1; \\frac{-||\\mu||^2}{2}\\right)\\mathbf{I} +\n",
    "  {}_{1}F_1\\left(1; \\frac{d}{2}+2; \\frac{-||\\mu||^2}{2}\\right)\\frac{1}{d+2}\n",
    "  \\mu\\mu^T\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu = \\frac{1}{\\sigma}s$ and ${}_{1}F_1\\left(a; b; c\\right)$\n",
    "is the hypergeometric confluent function\n",
    "(which is included in standard scientific computing packages). It can be seen\n",
    "that the result is a weigthed sum of the identity $\\mathbf{I}$ and the\n",
    "outer product of the standardized mean, $\\mu\\mu^T$.\n",
    "The derivation is provided in the companion notes.\n",
    "\n",
    "We next show an implementation of this formula, and compare the\n",
    "analytic results to empirical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5c68a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### IMPORT PACKAGES\n",
    "##############\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpmath as mpm  # Important to use mpmath for hyp1f1, scipy blows up\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f08e23",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### DEFINE THE FUNCTION TO COMPUTE THE SECOND MOMENT OF\n",
    "#### ISOTROPIC NOISE, BROADBAND NORM\n",
    "##############\n",
    "def isotropic_broadb_sm(s, sigma):\n",
    "    \"\"\" Estimate the second moment of a noisy normalized stimulus,\n",
    "    with isotropic white noise and broadband normalization.\n",
    "    s: Stimulus mean. shape nDim\n",
    "    sigma: Standar deviation of the isotropic noise\n",
    "    \"\"\"\n",
    "    df = int(s.shape[0])  # Get number of dimensions\n",
    "    sNorm = s/sigma  # Standardize the stimulus dividing it by sigma (\\mu)\n",
    "    nc = float(torch.sum(sNorm**2))  # non-centrality parameter, ||\\mu||^2\n",
    "    # Hypergeometric function for the term with the identity\n",
    "    hypFunNoise = torch.tensor(float(mpm.hyp1f1(1, df/2+1, -nc/2)))\n",
    "    # Hypergeometric function for the term with the mean\n",
    "    hypFunMean = torch.tensor(float(mpm.hyp1f1(1, df/2+2, -nc/2))) \n",
    "    # Get the outer product of the normalized stimulus, and multiply by weight\n",
    "    meanTerm = torch.einsum('a,b->ab', sNorm, sNorm) * 1/(df+2) * hypFunMean\n",
    "    # Multiply the identity matrix by weighting term \n",
    "    noiseTerm = torch.eye(df) * 1/df * hypFunNoise\n",
    "    # Add the two terms\n",
    "    expectedCov = (meanTerm + noiseTerm)\n",
    "    return expectedCov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e9c4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### COMPUTE ANALYTIC AND EMPIRICAL SECOND MOMENTS\n",
    "##############\n",
    "# All combinations of dimensions and noise below will be used\n",
    "nDim = torch.tensor([20, 50, 200]) # Vector with number of dimensions to use\n",
    "sigmaVec = torch.tensor([0.1, 0.5, 1, 2, 5]) # Vector with sigma values to use\n",
    "nFits = len(nDim) * len(sigmaVec)\n",
    "\n",
    "# Number of samples\n",
    "nSamplesRef = torch.tensor(2*10**5) # N Samples for reference empirical distribution\n",
    "nSamplesLow = torch.tensor(1000) # N Samples for noisy empirical estimation\n",
    "\n",
    "smDict = {'nDim': torch.zeros(nFits), 'sigma': torch.zeros(nFits),\n",
    "    'smAnalytic': [], 'smEmpRef': [], 'smEmpLow': []}\n",
    "ii = 0\n",
    "for d in range(len(nDim)):\n",
    "    for n in range(len(sigmaVec)):\n",
    "        df = nDim[d]  # Dimensions of vector\n",
    "        sigma = sigmaVec[n]   # Standard deviation of noise\n",
    "        # Use a sine function as the mean of the distribution\n",
    "        s = torch.sin(torch.linspace(0, 2*torch.pi, df))\n",
    "        # Store noise and dimension values\n",
    "        smDict['nDim'][ii] = df\n",
    "        smDict['sigma'][ii] = sigma\n",
    "        ### Calculate analytic second moment:\n",
    "        smAnalytic = isotropic_broadb_sm(s, sigma)\n",
    "        ### Calculate empirical reference distribution:\n",
    "        # Initialize distribution of samples\n",
    "        Cov = torch.eye(df)*(sigma**2)\n",
    "        # distribution of x = s + \\gamma\n",
    "        xDist = MultivariateNormal(loc=s, covariance_matrix=Cov)\n",
    "        # Make random samples\n",
    "        xSamples = xDist.rsample([nSamplesRef]) # for reference value\n",
    "        xSamplesLow = xDist.rsample([nSamplesLow]) # for noisy estimation\n",
    "        # Get normalizing factors for the samples\n",
    "        normRef = (torch.norm(xSamples, dim=1)**2)\n",
    "        normLow = (torch.norm(xSamplesLow, dim=1)**2)\n",
    "        # Compute empirical second moments\n",
    "        smEmpRef = torch.einsum('ni,n,nj->ij', xSamples, 1/normRef, xSamples) / nSamplesRef\n",
    "        smEmpLow = torch.einsum('ni,n,nj->ij', xSamplesLow, 1/normLow, xSamplesLow) / nSamplesLow\n",
    "        # Store second moments in dictionary\n",
    "        smDict['smAnalytic'].append(smAnalytic)\n",
    "        smDict['smEmpRef'].append(smEmpRef)\n",
    "        smDict['smEmpLow'].append(smEmpLow)\n",
    "        ii = ii+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb4c28",
   "metadata": {},
   "source": [
    "Below we plot the individual elements of the covariance\n",
    "matrices, obtained empirically through a large number of\n",
    "samples (X axis), obtained empirically with a small number\n",
    "of samples (red dots), or obtained analytically with the\n",
    "formula above (blue dots).\n",
    "\n",
    "We will see that the empirical estimates of the covariance\n",
    "can be very noisy, but that the analytic expression\n",
    "is practically identical to the large-sample reference empirical\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b2b149",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### COMPARE ANALYTIC METHOD TO LOW-SAMPLE EMPIRICAL ESTIMATION\n",
    "##############\n",
    "\n",
    "### Plot analytical vs empirical covariances\n",
    "nCols = len(nDim)\n",
    "nRows = len(sigmaVec)\n",
    "\n",
    "ii = 0\n",
    "for c in range(nCols):\n",
    "    for r in range(nRows):\n",
    "        # Extract values for this plot\n",
    "        sigma = smDict['sigma'][ii]\n",
    "        df = smDict['nDim'][ii]\n",
    "        smEmpRef = smDict['smEmpRef'][ii].reshape(int(df**2))\n",
    "        smEmpLow = smDict['smEmpLow'][ii].reshape(int(df**2))\n",
    "        smAnalytic = smDict['smAnalytic'][ii].reshape(int(df**2))\n",
    "        ax = plt.subplot(nCols, nRows, ii+1)\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        # scatter the values\n",
    "        plt.scatter(smEmpRef, smEmpLow, c='red', label=f'{nSamplesLow} samples', s=1.5)\n",
    "        plt.scatter(smEmpRef, smAnalytic, color='blue', label='Analytic', s=1.5)\n",
    "        # Add identity line\n",
    "        ax.axline((0,0), slope=1, color='black')\n",
    "        # Add names only on edge panels\n",
    "        if c==(nCols-1):\n",
    "            plt.xlabel('Empirical reference')\n",
    "        if r==0:\n",
    "            plt.ylabel('Estimation')\n",
    "        plt.title(f'd={int(df)}, $\\sigma$ = {sigma:.1f}', fontsize=11)\n",
    "        if c==(nCols-1) and r==(nRows-1):\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "        ii = ii+1\n",
    "# Add color code legend\n",
    "fig = plt.gcf()\n",
    "(lines, labels) = ax.get_legend_handles_labels()\n",
    "lines = lines[0:2]\n",
    "labels = labels[0:2]\n",
    "fig.legend(lines, labels, loc='upper right')\n",
    "fig.set_size_inches(10,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c1e6e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### For better visualization of the results, we\n",
    "### view the 3 second moment matrices for one of the cases\n",
    "\n",
    "# Select the noise and dimensions level\n",
    "noiseInd = 3\n",
    "dimInd = 2\n",
    "\n",
    "# Look for the index in the dictionary matching those levels\n",
    "ind = torch.logical_and(smDict['nDim']==nDim[dimInd], \\\n",
    "        smDict['sigma']==sigmaVec[noiseInd])\n",
    "ind = np.flatnonzero(ind)[0]\n",
    "\n",
    "# Plot the matrices\n",
    "plt.title('Second moment matrices')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(smDict['smAnalytic'][ind])\n",
    "plt.title(f'Analytic', fontsize=11)\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(f'Empirical ref', fontsize=11)\n",
    "plt.imshow(smDict['smEmpRef'][ind])\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(f'Empirical, {int(nSamplesLow)} samples', fontsize=11)\n",
    "plt.imshow(smDict['smEmpLow'][ind])\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(f'Second moments d={nDim[dimInd]}, $\\sigma$={sigmaVec[noiseInd]}')\n",
    "plt.setp(fig.get_axes(), xticks=[], yticks=[])\n",
    "fig.set_size_inches(7,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1ef6ac",
   "metadata": {},
   "source": [
    "# Non-isotropic Gaussian noise, flexible normalization\n",
    "\n",
    "A more general model for noisy normalized stimuli involves\n",
    "non-isotropic Gaussian noise, $\\gamma \\sim \\mathcal{N}(0, \\Psi)$\n",
    "where $\\Psi$ is any symmetric positive-semi-definite matrix, and\n",
    "flexible normalization, $\\frac{1}{g_{s,f}||s + \\gamma||}$,\n",
    "where $g_{s,f}$ is a normalizing factor that depends on the\n",
    "unnoisy, unnormalized stimulus $\\mathbf{s}$, and on a linear\n",
    "filter $\\mathbf{f}$.\n",
    "\n",
    "To estimate the second moment of a noisy, normalized stimulus\n",
    "under these conditions, the following approximation can be used\n",
    "(derived in the companion notes)\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbb{E}_{\\gamma}\\left( \\frac{XX^T}{||X||^2} \\right) \\approx\n",
    "    \\frac{1}{g_{s,f}^2} \\frac{\\mathbf{\\mu}_N}{\\mu_D} \\odot \\left( 1 -\n",
    "    \\frac{\\mathbf{\\Sigma}^{N,D}}{\\mathbf{\\mu}_N\\mu_D} + \\frac{Var(D)}{\\mu_D^2} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $X \\sim \\mathcal{N}(\\mathbf{s}, \\Psi)$, $\\odot$ indicates matrix-wise\n",
    "multiplication, divisions between matrices are taken element-wise, and\n",
    "$$\\mu_N = \\Psi + s s^T $$\n",
    "$$\\mu_{D} = (tr(\\Psi) + ||s||^2)$$\n",
    "$$Var(D) = \\left(2 tr(\\Psi^2) + 4 s^T \\Psi s\\right)$$\n",
    "$$\\Sigma^{N,D} = 2 \\left[\\Psi \\Psi + s s^T \\Psi +  \\Psi s s^T \\right]$$\n",
    "\n",
    "Note that while $\\mu_N \\in \\mathbb{R}^{d\\times d}$ and\n",
    "$\\Sigma^{N,D} \\in \\mathbb{R}^{d\\times d}$,\n",
    "$\\mu_D \\in \\mathbb{R}$ and $Var(D) \\in \\mathbb{R}$.\n",
    "\n",
    "In the notation above, $N$ indicates moments of the numerator\n",
    "inside the expectation (i.e. products of the form $X_iX_j$)\n",
    "and $D$ refers to moments of the denominator (i.e.\n",
    "$||X||^2$).\n",
    "\n",
    "We note that the stimulus-specific normalization factor\n",
    "$\\frac{1}{g_{s,f}^2}$ does not depend on $\\gamma$, and thus it can just be\n",
    "taken out of the expectation. Thus, stimulus-specific normalization\n",
    "can also be applied to the isotropic noise case from previous section.\n",
    "\n",
    "Below, we test the performance of this approximation to recover\n",
    "response second moment under these noise and normalization conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70faed63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### DEFINE THE FUNCTION TO COMPUTE THE SECOND MOMENT OF\n",
    "#### ISOTROPIC NOISE, BROADBAND NORM\n",
    "##############\n",
    "def general_flexible_sm(s, sigmaCov, normScale):\n",
    "    \"\"\" Estimate the second moment of a noisy normalized stimulus,\n",
    "    with any Gaussian noise covariance and flexible normalization.\n",
    "    s: Stimulus mean. shape nDim\n",
    "    sigmaCov: Covariance matrix of the noise. shape nDim x nDim\n",
    "    normScale: Scalar that scales normalization.\n",
    "    \"\"\"\n",
    "    # Compute some repeated quantities\n",
    "    ssOuter = torch.einsum('i,j->ij', s, s) # outer prod of stim with from previous section\n",
    "    sigmaCov2 = torch.matmul(sigmaCov,sigmaCov) # square of covariance matrix\n",
    "    # Mean of numerator\n",
    "    mun = sigmaCov + ssOuter\n",
    "    # Mean of denominator\n",
    "    mud = (sigmaCov.trace() + torch.einsum('i,i->', s, s))\n",
    "    # Variance of denominator\n",
    "    vard = (2*torch.trace(sigmaCov2) + 4*torch.einsum('i,ij,j->', s, sigmaCov, s))\n",
    "    # Correlation between numerator and denominator \n",
    "    covnd = 2 * (sigmaCov2 + torch.matmul(ssOuter, sigmaCov) + \\\n",
    "        torch.matmul(sigmaCov, ssOuter))\n",
    "    # Expected value of ratio quadratic forms\n",
    "    secondMoment = (1/normScale**2) * mun/mud * \\\n",
    "        (1 - covnd/(mun*mud) + vard/(mud**2))\n",
    "    return secondMoment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267b1ac",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### DEFINE A FUNCTION TO CREATE DIFFERENT TYPES OF COVARIANCE MATRICES\n",
    "##############\n",
    "\n",
    "# Make a function to create custom covariance matrices\n",
    "def make_cov(covType, sigmaScale, df, decay=1, baseline=0):\n",
    "    \"\"\" Generate a covariance matrix with specified properties.\n",
    "    covType: Indicates the type of process that the covariance\n",
    "        describes. Is a string that can take values:\n",
    "        -'random': Random diagonal and off-diagonal elements.\n",
    "        -'diagonal': Random diagonal elements. Off-diagonal elements are 0.\n",
    "        -'scaled': Diagonal elements = sigmaScale + baseline. Off diagonal\n",
    "            elements are 0. Can be used for Poisson-like noise.\n",
    "        -'decaying': Diagonal elements = sigmaScale. Off-diagonal elements\n",
    "            decay exponentially with distance to diagonal\n",
    "    sigmaScale: For most covTypes, it is a scalar that multiplies the\n",
    "        resulting covariance matrix. For covType='scaled', it is a vector\n",
    "        that is used as the diagonal (+ baseline).\n",
    "    df: Dimensionality of the matrix.\n",
    "    decay: Rate of decay of the 'decaying' type of matrix. Is a scalar.\n",
    "    baseline: Constant added to 'scaled' covariance matrix diagonal elements.\n",
    "    \"\"\"\n",
    "    if covType=='random':\n",
    "        isPD = False\n",
    "        while not isPD:\n",
    "            randMat = torch.randn(int(df), int(df))  # Make random matrix\n",
    "            covRand = torch.cov(randMat)  # Turn it into positive-semidefinite matrix\n",
    "            diagW = torch.rand(1)  # Sample a relative weight of diagonal-off diagonal elements\n",
    "            sigmaCov = ((1-diagW) * covRand + diagW * torch.diag(covRand.diag())) * sigma\n",
    "            eigVals = torch.real(torch.linalg.eigvals(sigmaCov))\n",
    "            isPD = all(eigVals > 0)\n",
    "    if covType=='diagonal':\n",
    "        sigmaCov = torch.diag((torch.rand(df)+0.5)*2-1)\n",
    "    if covType=='scaled':\n",
    "        sigmaCov = torch.diag(torch.abs(sigmaScale)+baseline)\n",
    "    if covType=='decaying':\n",
    "        cov = torch.zeros((df, df))\n",
    "        indRow = torch.reshape(torch.linspace(0, 1, df), (df,1)).repeat(1,df)\n",
    "        indCol = torch.reshape(torch.linspace(0, 1, df), (1,df)).repeat(df,1)\n",
    "        diagDist = torch.abs(indRow - indCol)\n",
    "        sigmaCov = torch.exp(-(diagDist*decay)) * sigmaScale\n",
    "    return sigmaCov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54f98c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### COMPUTE ANALYTIC AND EMPIRICAL SECOND MOMENTS\n",
    "##############\n",
    "\n",
    "# All combinations of dimensions and noise below will be used\n",
    "nDim = torch.tensor([10, 50, 200])  # Vector with number of dimensions to use\n",
    "sigmaVec = torch.tensor([0.2, 1, 3])  # Vector with sigma values to use to scale cov\n",
    "gVec = torch.tensor([0.7]) # normalizing factor\n",
    "\n",
    "# covariance parameters\n",
    "covType = 'decaying'\n",
    "decay = 5\n",
    "baseline = 0.05\n",
    "\n",
    "# Number of samples\n",
    "nSamplesRef = torch.tensor(10**6)  # N Samples for reference empirical distribution\n",
    "nSamplesLow = torch.tensor(1000)     # N Samples for noisy empirical estimation\n",
    "\n",
    "# Number of different random variables to test\n",
    "nFits = len(nDim) * len(sigmaVec) * len(gVec)\n",
    "\n",
    "smDict = {'nDim': torch.zeros(nFits), 'sigma': torch.zeros(nFits), 'g': torch.zeros(nFits),\n",
    "        'sigmaCov': [], 'smAnalytic': [], 'smEmpRef': [], 'smEmpLow': []}\n",
    "ii=0\n",
    "for d in range(len(nDim)):\n",
    "    for n in range(len(sigmaVec)):\n",
    "        for j in range(len(gVec)):\n",
    "            df = int(nDim[d])  # Dimensions of vector\n",
    "            sigma = sigmaVec[n]   # Standard deviation of noise\n",
    "            g = gVec[j]\n",
    "            # Use a sine function as the mean of the distribution\n",
    "            s = torch.sin(torch.linspace(0, 2*torch.pi, df))\n",
    "            # Store noise and dimension values\n",
    "            smDict['nDim'][ii] = df\n",
    "            smDict['sigma'][ii] = sigma\n",
    "            smDict['g'][ii] = g\n",
    "            # Make a random covariance matrix as requested\n",
    "            if covType=='scaled':\n",
    "                sigma = sigma*s\n",
    "            sigmaCov = make_cov(covType=covType, sigmaScale=sigma, df=df,\n",
    "                decay=decay, baseline=baseline)\n",
    "            smDict['sigmaCov'].append(sigmaCov)\n",
    "            ### Calculate analytic second moment:\n",
    "            smAnalytic = general_flexible_sm(s, sigmaCov, g)\n",
    "            ### Calculate empirical reference distribution:\n",
    "            # Initialize distribution of samples\n",
    "            # distribution of x = s + \\gamma\n",
    "            xDist = MultivariateNormal(loc=s, covariance_matrix=sigmaCov)\n",
    "            # Make random samples\n",
    "            xSamples = xDist.rsample([int(nSamplesRef)]) # for reference value\n",
    "            xSamplesLow = xDist.rsample([int(nSamplesLow)]) # for noisy estimation\n",
    "            # Get normalizing factors for the samples\n",
    "            normRef = (torch.norm(xSamples, dim=1)*g)**2\n",
    "            normLow = (torch.norm(xSamplesLow, dim=1)*g)**2\n",
    "            # Compute empirical second moments\n",
    "            smEmpRef = torch.einsum('ni,n,nj->ij', xSamples, 1/normRef, xSamples) / nSamplesRef\n",
    "            smEmpLow = torch.einsum('ni,n,nj->ij', xSamplesLow, 1/normLow, xSamplesLow) / nSamplesLow\n",
    "            # Store second moments in dictionary\n",
    "            smDict['smAnalytic'].append(smAnalytic)\n",
    "            smDict['smEmpRef'].append(smEmpRef)\n",
    "            smDict['smEmpLow'].append(smEmpLow)\n",
    "            ii = ii+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3856c0ed",
   "metadata": {},
   "source": [
    "Below we compare empirical and analytic results to an\n",
    "empirical reference, like in previous section.\n",
    "\n",
    "We see that the empirical estimates of the covariance\n",
    "can be noisy, and that the analytic expression works\n",
    "well for most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1045ca1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### COMPARE ANALYTIC METHOD TO LOW-SAMPLE EMPIRICAL ESTIMATION\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46506e0f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Plot analytical vs empirical covariances\n",
    "nRows = len(nDim)\n",
    "nCols = len(sigmaVec)\n",
    "\n",
    "ii=0\n",
    "for r in range(nRows):\n",
    "    for c in range(nCols):\n",
    "        # Extract values for this plot\n",
    "        sigma = smDict['sigma'][ii]\n",
    "        df = int(smDict['nDim'][ii])\n",
    "        nonRepeatedInds = torch.nonzero(torch.tril(torch.ones(df,df)).reshape(df**2))\n",
    "        smEmpRef = smDict['smEmpRef'][ii].reshape(df**2)[nonRepeatedInds]\n",
    "        smEmpLow = smDict['smEmpLow'][ii].reshape(df**2)[nonRepeatedInds]\n",
    "        smAnalytic = smDict['smAnalytic'][ii].reshape(df**2)[nonRepeatedInds]\n",
    "        ax = plt.subplot(nRows, nCols, ii+1)\n",
    "        # scatter the values\n",
    "        plt.scatter(smEmpRef, smEmpLow, c='red', label=f'{nSamplesLow} samples',\n",
    "                s=1.5, alpha=0.2)\n",
    "        plt.scatter(smEmpRef, smAnalytic, color='blue', label='Analytic',\n",
    "                s=1, alpha=0.2)\n",
    "        # Add identity line\n",
    "        ax.axline((0,0), slope=1, color='black')\n",
    "        # Add names only on edge panels\n",
    "        if r==(nRows-1):\n",
    "            plt.xlabel('Empirical reference')\n",
    "        else:\n",
    "            ax.set_xticks([], [])\n",
    "        if c==0:\n",
    "            plt.ylabel('Estimation')\n",
    "        else:\n",
    "            ax.set_yticks([], [])\n",
    "        plt.title(f'd={int(df)}, $\\sigma$ = {sigma:.2f}', fontsize=11)\n",
    "        if r==(nRows-1) and c==(nCols-1):\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "        ii = ii+1\n",
    "# Add color code legend\n",
    "fig = plt.gcf()\n",
    "(lines, labels) = ax.get_legend_handles_labels()\n",
    "lines = lines[0:2]\n",
    "labels = labels[0:2]\n",
    "fig.legend(lines, labels, loc='upper right')\n",
    "fig.set_size_inches(16,14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed409e",
   "metadata": {},
   "source": [
    "Next, we show, for one of the examples given above (i.e.\n",
    "one noise level, and one dimensionality) the empirical\n",
    "reference matrix, our approximation, and the low-sample\n",
    "empirical approximation. We also show a matrix with the\n",
    "differences between analytic and empirical approximation\n",
    "and the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9656ec6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### For better visualization of the results, we\n",
    "### view the 3 second moment matrices for one of the cases\n",
    "# Select the noise and dimensions level\n",
    "noiseInd = 1\n",
    "dimInd = 1\n",
    "\n",
    "# Look for the index in the dictionary matching those levels\n",
    "ind = torch.logical_and(smDict['nDim']==nDim[dimInd], \\\n",
    "        smDict['sigma']==sigmaVec[noiseInd])\n",
    "ind = np.flatnonzero(ind)[0]\n",
    "\n",
    "maxErr1 = torch.max(smDict['smEmpRef'][ind]-smDict['smAnalytic'][ind])\n",
    "maxErr2 = torch.max(smDict['smEmpRef'][ind]-smDict['smEmpLow'][ind])\n",
    "maxErr = max((maxErr1, maxErr2))\n",
    "\n",
    "# Plot the matrices\n",
    "# Analytic second moment matrix\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(smDict['smAnalytic'][ind])\n",
    "plt.title(f'Analytic', fontsize=11)\n",
    "# Diff Reference - Empirical\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(smDict['smEmpRef'][ind]-smDict['smAnalytic'][ind], cmap='bwr')\n",
    "plt.clim(-maxErr, maxErr)\n",
    "plt.title(f'Reference - Analytic', fontsize=11)\n",
    "#  Empirical reference\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(smDict['smEmpRef'][ind])\n",
    "plt.title(f'Empirical ref', fontsize=11)\n",
    "#  Empirical low sample\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(smDict['smEmpLow'][ind])\n",
    "plt.title(f'Empirical {int(nSamplesLow)} samples', fontsize=11)\n",
    "#  Diff Reference - :ow sample\n",
    "plt.subplot(2,3,6)\n",
    "plt.imshow(smDict['smEmpRef'][ind] - smDict['smEmpLow'][ind], cmap='bwr')\n",
    "plt.clim(-maxErr, maxErr)\n",
    "plt.title(f'Reference - Low sample', fontsize=11)\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(f'Second moments d={int(nDim[dimInd])}, $\\sigma$={sigmaVec[noiseInd]:.2f}')\n",
    "plt.setp(fig.get_axes(), xticks=[], yticks=[])\n",
    "fig.set_size_inches(9,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8fffc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Let's see the random covariance matrices that we sampled for each case\n",
    "ii = 0\n",
    "for r in range(nRows):\n",
    "    for c in range(nCols):\n",
    "        # Extract values for this plot\n",
    "        sigma = smDict['sigma'][ii]\n",
    "        df = smDict['nDim'][ii]\n",
    "        sigmaCov = smDict['sigmaCov'][ii]\n",
    "        ax = plt.subplot(nRows, nCols, ii+1)\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        plt.imshow(sigmaCov)\n",
    "        plt.title(f'd={int(df)}, $\\sigma$ = {sigma:.2f}', fontsize=11)\n",
    "        ii = ii+1\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(f'Noise covariances for each dimension and $\\sigma$')\n",
    "plt.setp(fig.get_axes(), xticks=[], yticks=[])\n",
    "fig.set_size_inches(14,12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e91c5",
   "metadata": {},
   "source": [
    "# Covariance of naturalistic image dataset\n",
    "\n",
    "Next, we move beyond the single, artificial stimulus that has\n",
    "been used so far, and apply the analytic second moment estimation\n",
    "to estimate the covariance of a real image dataset.\n",
    "\n",
    "We use a naturalistic dataset of binocular video patches of 3D motion.\n",
    "The dataset is composed of a set of non-noisy, unnormalized contrast\n",
    "stimuli, $\\mathbf{s}_v \\in \\mathbb{R}^d$, where\n",
    "$v \\in \\{1, ..., n\\}$, and $n$ is the number of stimuli in the dataset.\n",
    "The stimuli in our dataset are 1D binocular videos. They contain the\n",
    "input corresponding to two retinas, with a number nPixels of horizontal\n",
    "pixels, and with a number nTimesteps of time steps.\n",
    "\n",
    "We want to estimate the covariance of the dataset of noisy, normalized\n",
    "stimuli (as defined above)\n",
    "$\\mathbf{c}_v \\in \\mathbb{R}^d$ with $v \\in \\{1, ..., n\\}$, and\n",
    "isotropic noise $\\gamma \\sim \\mathcal{N}(0,\\mathbf{I}\\sigma^2)$.\n",
    "The expression for the covariance can be reduced to the\n",
    "mean of the covariances of the noisy stimuli across the dataset\n",
    "(see accompanying notes for the derivation)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbb{E}_{\\gamma,\\mathbf{s}}\\left(\\mathbf{c}\\mathbf{c}^T\\right) =\n",
    "    \\frac{1}{n} \\sum_{v=1}^{v=n} \\mathbb{E}_{\\gamma}\\left(\\mathbf{c}_v\\mathbf{c}_v^T\\right)\n",
    "\\end{equation}\n",
    "\n",
    "In this section, we use the formula for the expected\n",
    "second moment under isotropic noise and broadband\n",
    "normalization (presented in section 1 of the notebook)\n",
    "to estimate the second moment of each stimulus,\n",
    "\\mathbb{E}_{\\gamma}\\left(\\mathbf{c}_v\\mathbf{c}_v^T\\right)\n",
    "and then put these together into the full dataset second moment as\n",
    "shown above.\n",
    "\n",
    "We also compare the results of our analytic calculation to\n",
    "the result of low-sample empirical estimation of the second moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98611a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT PACKAGES AND AMA UTILITIES\n",
    "#########################\n",
    "import scipy.io as spio\n",
    "import time\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776bbf61",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# COMMENT THIS CELL FOR GOOGLE COLAB EXECUTION\n",
    "#import ama_library.ama_utilities as au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b824bc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### UNCOMMENT THIS CELL FOR GOOGLE COLAB EXECUTION\n",
    "!pip install geotorch\n",
    "import geotorch\n",
    "!pip install git+https://github.com/dherrera1911/accuracy_maximization_analysis.git\n",
    "import ama_library.ama_utilities as au\n",
    "!wget -O ./data/amaInput_12dir_3speed_0stdDsp_train.mat https://drive.google.com/file/d/1m7BXFZFe0ppsHhURFbhqaCqHR3vTbD6V/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718a97a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT AND PREPROCESS THE STIMULUS DATASET\n",
    "#########################\n",
    "# Load ama struct from .mat file into Python\n",
    "data = spio.loadmat('./data/amaInput_12dir_3speed_0stdDsp_train.mat')\n",
    "# Extract contrast normalized, noisy stimulus\n",
    "s = data.get(\"Iret\")\n",
    "s = torch.from_numpy(s)\n",
    "s = s.transpose(0,1)\n",
    "s = s.float()\n",
    "# We turn the images into contrast stimuli\n",
    "sWeb = au.contrast_stim(s)\n",
    "# Extract the vector indicating category of each stimulus row\n",
    "ctgInd = data.get(\"ctgIndMotion\")\n",
    "ctgInd = torch.tensor(ctgInd)\n",
    "ctgInd = ctgInd.flatten()\n",
    "ctgInd = ctgInd-1       # convert to python indexing (subtract 1)\n",
    "ctgInd = ctgInd.type(torch.LongTensor)  # convert to torch integer\n",
    "nCtg = int(ctgInd.max()+1)\n",
    "# Extract the values of the latent variable\n",
    "ctgVal = data.get(\"Xmotion\")\n",
    "ctgVal = torch.from_numpy(ctgVal)\n",
    "# Extract some properties of the dataset\n",
    "htz = data.get('smpPerSec')\n",
    "nTimesteps = int(data.get('durationMs')/(1000)*htz)\n",
    "nPixels = len(data.get('smpPosDegX'))\n",
    "nStim = sWeb.shape[0]\n",
    "df = s.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c13f84",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# VISUALIZE A STIMULUS\n",
    "#########################\n",
    "nRand = torch.randint(high=nStim-1, size=(1,1))\n",
    "ax = au.view_filters_bino_video(sWeb[nRand,:].unsqueeze(0),\n",
    "        frames=nTimesteps, pixels=nPixels)\n",
    "ax.axes.xaxis.set_visible(True)\n",
    "ax.axes.yaxis.set_visible(True)\n",
    "plt.xlabel('Pixels')\n",
    "plt.ylabel('Time')\n",
    "plt.title('Left eye                   Right eye')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abeb7eb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# SELECT PARAMETERS FOR THE COVARIANCE ESTIMATION PROCEDURE\n",
    "#########################\n",
    "# Choose the std for the isotropic gaussian noise\n",
    "pixelSigma = 0.2\n",
    "# Choose one category of the random variable whose second moment\n",
    "# to estimate\n",
    "ctg = 14\n",
    "# Number of samples per stimulus for the empirical 'true' reference\n",
    "samplesPerStimRef = 700\n",
    "# Number of samples per stimulus for the empirical low-samples reference\n",
    "samplesPerStimLow = 1  # Samples per each stim, low sample estimate\n",
    "#\n",
    "# Extract the stimuli of this category\n",
    "sCtg = sWeb[torch.nonzero(ctgInd==ctg),:]. squeeze(1)\n",
    "# Get number of total stim and samples\n",
    "nCtgStim = sCtg.shape[0]\n",
    "nSamplesRef = nCtgStim * samplesPerStimRef\n",
    "nSamplesLow = nCtgStim * samplesPerStimLow\n",
    "# Generate the Covariance matrix\n",
    "noiseCov = torch.eye(df)*pixelSigma**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b22cb3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# COMPUTE ANALYTIC ESTIMATE OF SECOND MOMENT FOR THE CATEGORY\n",
    "#########################\n",
    "#\n",
    "# Get analytic estimate and time it\n",
    "start = time.time()\n",
    "analyticCov = au.isotropic_broadb_sm_batch(sCtg, sigma=pixelSigma)\n",
    "end = time.time()\n",
    "print(f'Analytic took: {end-start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7c8b02",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# COMPUTE EMPIRICAL ESTIMATES\n",
    "#########################\n",
    "#\n",
    "# Initialize a torch multivariante normal distribution for\n",
    "# the noise with the required covariance\n",
    "gamma = MultivariateNormal(loc=torch.zeros(df),\n",
    "        covariance_matrix=noiseCov)\n",
    "#\n",
    "# Make the samples of the noisy, normalized dataset, with\n",
    "# the reference number of samples\n",
    "start = time.time()\n",
    "start = time.time()\n",
    "gamma.rsample([int(nSamplesRef)])\n",
    "end = time.time()\n",
    "end - start\n",
    "xSamples = sCtg.repeat(samplesPerStimRef,1) + gamma.rsample([int(nSamplesRef)])\n",
    "# Get normalization factor for each sample\n",
    "xSamplesNorm = xSamples.norm(dim=1)\n",
    "# Normalize each random sample\n",
    "xSamples = torch.einsum('nd,n->nd', xSamples, 1/xSamplesNorm)\n",
    "# Compute the second moment matrix of the samples\n",
    "refCov = torch.einsum('nd,nb->db', xSamples, xSamples) * 1/nSamplesRef\n",
    "end = time.time()\n",
    "print(f'Reference took: {end-start}')\n",
    "#\n",
    "# Do the same for the low-sample empirical estimation\n",
    "start = time.time()\n",
    "xSamplesLow = sCtg.repeat(samplesPerStimLow,1) + gamma.rsample([int(nSamplesLow)])\n",
    "# Normalization factor\n",
    "xSamplesLowNorm = xSamplesLow.norm(dim=1)\n",
    "# Normalize each random sample\n",
    "xSamplesLow = torch.einsum('nd,n->nd', xSamplesLow, 1/xSamplesLowNorm)\n",
    "# Compute the second moment matrix of the samples\n",
    "lowEmpCov = torch.einsum('nd,nb->db', xSamplesLow, xSamplesLow) * 1/nSamplesLow\n",
    "end = time.time()\n",
    "print(f'Low-samples took: {end-start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd6b85",
   "metadata": {},
   "source": [
    "We now compare the results of the empirical approximation and\n",
    "the analytically computed second moment. First we visualize\n",
    "the three obtained matrices, and a visualization of the\n",
    "estimation error for each matrix element. Then we show a\n",
    "scatter plot with the reference empirical value in the X\n",
    "axis, and the analytic and low-sample values on the Y axis.\n",
    "The spread of the low-sample estimation shows the\n",
    "approximation error of this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14319b96",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# VISUALIZE THE THREE COVARIANCE MATRICES, AND\n",
    "# THE ESTIMATION ERROR OF EACH ELEMENT\n",
    "#########################\n",
    "#\n",
    "# Compute the maximum errors to get a common color scale\n",
    "maxErr1 = torch.max(refCov - analyticCov)\n",
    "maxErr2 = torch.max(refCov - lowEmpCov)\n",
    "maxErr = max((maxErr1, maxErr2))\n",
    "#\n",
    "# Plot the matrices\n",
    "# Analytic second moment matrix\n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(analyticCov)\n",
    "plt.title(f'Analytic', fontsize=11)\n",
    "# Diff Reference - Empirical\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(refCov - analyticCov, cmap='bwr')\n",
    "plt.clim(-maxErr, maxErr)\n",
    "plt.title(f'Reference - Analytic', fontsize=11)\n",
    "#  Empirical reference\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(refCov)\n",
    "plt.title(f'Empirical ref', fontsize=11)\n",
    "#  Empirical low sample\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(lowEmpCov)\n",
    "plt.title(f'Empirical {int(nSamplesLow)} samples', fontsize=11)\n",
    "#  Diff Reference - :ow sample\n",
    "plt.subplot(2,3,6)\n",
    "plt.imshow(refCov - lowEmpCov, cmap='bwr')\n",
    "plt.clim(-maxErr, maxErr)\n",
    "plt.title(f'Reference - Low sample', fontsize=11)\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(f'Second moments d={int(df)}, $\\sigma$={pixelSigma}')\n",
    "plt.setp(fig.get_axes(), xticks=[], yticks=[])\n",
    "fig.set_size_inches(7,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3f403",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# VISUALIZE THE SCATTER PLOT OF EACH ELEMENTS ESTIMATES\n",
    "# FOR THE TWO ESTIMATION METHODS\n",
    "#########################\n",
    "#\n",
    "# Get the indices of the non-repeated matrix elements (because of symmetry)\n",
    "nonRepeatedInds = torch.nonzero(torch.tril(torch.ones(df,df)).reshape(df**2))\n",
    "# Get vectors with the matrix elements for the 3 matrices\n",
    "smEmpRef = refCov.reshape(df**2)[nonRepeatedInds]\n",
    "smEmpLow = lowEmpCov.reshape(df**2)[nonRepeatedInds]\n",
    "smAnalytic = analyticCov.reshape(df**2)[nonRepeatedInds]\n",
    "# Do scatter plot\n",
    "plt.scatter(smEmpRef, smEmpLow, c='red', label=f'{nSamplesLow} samples',\n",
    "        s=1.2, alpha=0.2)\n",
    "plt.scatter(smEmpRef, smAnalytic, color='blue', label='Analytic',\n",
    "        s=1.2, alpha=0.2)\n",
    "# Add identity line\n",
    "plt.axline((0,0), slope=1, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e16bb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# COMPUTE AN APPROXIMATION TO THE STIMULUS MEAN, AND THE\n",
    "# EMPIRICAL REFERENCE TO COMPARE TO\n",
    "#########################\n",
    "#\n",
    "# Compute the mean of the normalized, noisy stimuli\n",
    "refMean = xSamples.mean(dim=0)\n",
    "# Use naive normalizing factor, the non-noisy stim inverse norm\n",
    "normFactor1 = sCtg.norm(dim=1)\n",
    "sNorm = torch.einsum('nb,n->nb', sCtg, 1/normFactor1)\n",
    "meanApprox = sNorm.mean(dim=0) # mean of noisy normalized stim\n",
    "# Use inverse-chi-square normalizing factor\n",
    "normFactor2 = inv_ncx_batch(mu=sCtg, sigma=pixelSigma)\n",
    "meanApprox2 = torch.mean(torch.einsum('nb,n->nb', sCtg, normFactor2), dim=0)\n",
    "\n",
    "plt.plot(meanApprox, color='blue')\n",
    "plt.plot(meanApprox2, color='red')\n",
    "plt.plot(refMean, color='black')\n",
    "plt.show()\n",
    "\n",
    "ax = au.view_filters_bino_video(refMean.unsqueeze(0),\n",
    "        frames=nTimesteps, pixels=nPixels)\n",
    "ax.axes.xaxis.set_visible(True)\n",
    "ax.axes.yaxis.set_visible(True)\n",
    "plt.xlabel('Pixels')\n",
    "plt.ylabel('Time')\n",
    "plt.title('Left eye                   Right eye')\n",
    "plt.show()\n",
    "\n",
    "# Generate the Covariance matrix and torch distribution\n",
    "#Cov = torch.eye(df)*pixelSigma**2\n",
    "#gamma = MultivariateNormal(loc=torch.zeros(df), covariance_matrix=Cov)\n",
    "#analyticCov = torch.zeros(nCtg, df, df)\n",
    "#analyticDet = torch.zeros(nCtg)\n",
    "#lowEmpCov = torch.zeros(nCtg, df, df)\n",
    "#lowEmpDet = torch.zeros(nCtg)\n",
    "#for ctg in range(nCtg):\n",
    "#    # Extract the stimuli of this category\n",
    "#    sCtg = sWeb[torch.nonzero(ctgInd==ctg),:]. squeeze(1)\n",
    "#    analyticCov[ctg,:,:] = au.isotropic_broadb_sm_batch(sCtg, sigma=pixelSigma)\n",
    "#    analyticDet[ctg] = torch.linalg.det(analyticCov[ctg,:,:])\n",
    "#    xSamplesLow = sCtg.repeat(samplesPerStimLow,1) + \\\n",
    "#        gamma.rsample([int(nSamplesLow)]) # for noisy estimation\n",
    "#    xSamplesLowNorm = xSamplesLow.norm(dim=1) # Normalization factor\n",
    "#    xSamplesLow = torch.einsum('nd,n->nd', xSamplesLow, 1/xSamplesLowNorm)\n",
    "#    lowEmpCov[ctg,:,:] = torch.einsum('nd,nb->db', xSamplesLow, xSamplesLow) * \\\n",
    "#        1/nSamplesLow\n",
    "#    lowEmpDet[ctg] = torch.linalg.det(lowEmpCov[ctg,:,:])\n",
    "#\n",
    "#\n",
    "#ev = torch.real(torch.linalg.eigvals(analyticCov[10,:,:]))\n",
    "#evm = mpm.matrix(ev)\n",
    "#\n",
    "#\n",
    "#\n",
    "#plt.plot(analyticDet)\n",
    "#plt.show()\n",
    "#\n",
    "#plt.plot(lowEmpDet)\n",
    "#plt.show()\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#ctgPlot = np.arange(1, nCtg, 4)\n",
    "#nCol = len(ctgPlot)\n",
    "#for nc in range(nCol):\n",
    "#    ctg = ctgPlot[nc]\n",
    "#    plt.subplot(2, nCol, 1+nc)\n",
    "#    plt.imshow(analyticCov[ctg])\n",
    "#    plt.subplot(2, nCol, 1+nc+nCol)\n",
    "#    plt.imshow(lowEmpCov[ctg])\n",
    "#plt.show()\n",
    "#"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
