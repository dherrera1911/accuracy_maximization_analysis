{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbb741d",
   "metadata": {},
   "source": [
    "# Approximating the covariance of noisy, normalized, neural responses\n",
    "\n",
    "Neuronal responses are noisy (i.e. there is variability in the\n",
    "responses to a repeated stimulus). Also, neuronal responses\n",
    "are usually divisively normalized by the pooled responses\n",
    "of other neurons. Thus, when modeling neuronal responses, it\n",
    "is convenient to apply these two pre-processing steps: additive\n",
    "noise and normalization.\n",
    "\n",
    "In the simplest case, noise is modeled as an isotropic Gaussian\n",
    "around the mean response, and normalization is broadband\n",
    "(i.e. the population response vector is divided by its norm).\n",
    "This is expressed in the following formula:\n",
    "\n",
    "$$c = \\frac{s + \\gamma}{||s + \\gamma||}$$\n",
    "\n",
    "where $s \\in \\mathbb{R}^d$ is the expected value of the\n",
    "population response vector, with $d$ dimensions,\n",
    "$\\gamma \\sim \\mathcal{N}(0,\\mathbf{I}\\sigma^2), \\gamma \\in \\mathbb{R}^d$\n",
    "is a sample of multivariate white noise, and $c \\in \\mathbb{R}^d$ is the\n",
    "noisy, normalized, population response.\n",
    "\n",
    "When studying the statistics of responses of model neurons\n",
    "to natural signals (e.g. for building probabilistic response\n",
    "models), it is thus desirable to compute the statistics\n",
    "of the noisy, normalized responses.\n",
    "\n",
    "Here, we present analytic formulas to compute the covariances\n",
    "of noisy, normalized responses, from the noiseless \n",
    "unnormalized responses. We first present the formula\n",
    "to obtain the second moment of the noisy normalized responses\n",
    "for single stimuli. Then, we use these to compute\n",
    "the covariance of the noisy normalized responses to a\n",
    "set of natural images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c01dc92",
   "metadata": {},
   "source": [
    "# Isotropic Gaussian noise and broadband normalization\n",
    "\n",
    "In the case of $c = \\frac{s + \\gamma}{||s + \\gamma||}$\n",
    "with $\\gamma \\sim \\mathcal{N}(0, \\mathbf{I}\\sigma^2)$, there\n",
    "is an exact formula to compute the second moment\n",
    "$\\mathbb{E}_{\\gamma}(cc^T)$\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbb{E}_{\\gamma}\\left[cc^T \\right] =\n",
    "  \\frac{1}{d} {}_{1}F_1\\left(1; \\frac{d}{2}+1; \\frac{-||\\mu||^2}{2}\\right)\\mathbf{I} +\n",
    "  {}_{1}F_1\\left(1; \\frac{d}{2}+2; \\frac{-||\\mu||^2}{2}\\right)\\frac{1}{d+2}\n",
    "  \\mu\\mu^T\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mu = \\frac{1}{\\sigma}s$ and ${}_{1}F_1\\left(a; b; c\\right)$\n",
    "is the hypergeometric confluent function\n",
    "(which is included in standard scientific computing packages). It can be seen\n",
    "that the result is a weigthed sum of the identity $\\mathbf{I}$ and the\n",
    "outer product of the standardized mean, $\\mu\\mu^T$.\n",
    "The derivation is provided in the companion notes.\n",
    "\n",
    "We next show an implementation of this formula, and compare the\n",
    "analytic results to empirical results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ba7559",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### IMPORT PACKAGES\n",
    "##############\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpmath as mpm  # Important to use mpmath for hyp1f1, scipy blows up\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b64bb88",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### DEFINE THE FUNCTION TO COMPUTE THE SECOND MOMENT OF\n",
    "#### ISOTROPIC NOISE, BROADBAND NORM\n",
    "##############\n",
    "def isotropic_broadb_sm(s, sigma):\n",
    "    \"\"\" Estimate the second moment of a noisy normalized stimulus,\n",
    "    with isotropic white noise and broadband normalization.\n",
    "    s: Stimulus mean. shape nDim\n",
    "    sigma: Standar deviation of the isotropic noise\n",
    "    \"\"\"\n",
    "    df = int(s.shape[0])  # Get number of dimensions\n",
    "    sNorm = s/sigma  # Standardize the stimulus dividing it by sigma (\\mu)\n",
    "    nc = float(torch.sum(sNorm**2)) # non-centrality parameter, ||\\mu||^2\n",
    "    # Hypergeometric function for the term with the identity\n",
    "    hypFunNoise = torch.tensor(float(mpm.hyp1f1(1, df/2+1, -nc/2)))\n",
    "    # Hypergeometric function for the term with the mean\n",
    "    hypFunMean = torch.tensor(float(mpm.hyp1f1(1, df/2+2, -nc/2))) \n",
    "    # Get the outer product of the normalized stimulus, and multiply by weight\n",
    "    meanTerm = torch.einsum('a,b->ab', sNorm, sNorm) * 1/(df+2) * hypFunMean\n",
    "    # Multiply the identity matrix by weighting term \n",
    "    noiseTerm = torch.eye(df) * 1/df * hypFunNoise\n",
    "    # Add the two terms\n",
    "    expectedCov = (meanTerm + noiseTerm)\n",
    "    return expectedCov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f86845",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### COMPUTE ANALYTIC AND EMPIRICAL SECOND MOMENTS\n",
    "##############\n",
    "# All combinations of dimensions and noise below will be used\n",
    "nDim = torch.tensor([20, 50, 200]) # Vector with number of dimensions to use\n",
    "sigmaVec = torch.tensor([0.1, 0.5, 1, 2, 5]) # Vector with sigma values to use\n",
    "nFits = len(nDim) * len(sigmaVec)\n",
    "\n",
    "nSamplesRef = torch.tensor(2*10**5) # N Samples for reference empirical distribution\n",
    "nSamplesLow = torch.tensor(1000) # N Samples for noisy empirical estimation\n",
    "\n",
    "smDict = {'nDim': torch.zeros(nFits), 'sigma': torch.zeros(nFits),\n",
    "        'smAnalytic': [], 'smEmpRef': [], 'smEmpLow': []}\n",
    "ii=0\n",
    "for d in range(len(nDim)):\n",
    "    for n in range(len(sigmaVec)):\n",
    "        df = nDim[d]  # Dimensions of vector\n",
    "        sigma = sigmaVec[n]   # Standard deviation of noise\n",
    "        # Use a sine function as the mean of the distribution\n",
    "        s = torch.sin(torch.linspace(0, 2*torch.pi, df))\n",
    "        # Store noise and dimension values\n",
    "        smDict['nDim'][ii] = df\n",
    "        smDict['sigma'][ii] = sigma\n",
    "        ### Calculate analytic second moment:\n",
    "        smAnalytic = isotropic_broadb_sm(s, sigma)\n",
    "        ### Calculate empirical reference distribution:\n",
    "        # Initialize distribution of samples\n",
    "        Cov = torch.eye(df)*(sigma**2)\n",
    "        # distribution of x = s + \\gamma\n",
    "        xDist = MultivariateNormal(loc=s, covariance_matrix=Cov)\n",
    "        # Make random samples\n",
    "        xSamples = xDist.rsample([nSamplesRef]) # for reference value\n",
    "        xSamplesLow = xDist.rsample([nSamplesLow]) # for noisy estimation\n",
    "        # Get normalizing factors for the samples\n",
    "        normRef = (torch.norm(xSamples, dim=1)**2)\n",
    "        normLow = (torch.norm(xSamplesLow, dim=1)**2)\n",
    "        # Compute empirical second moments\n",
    "        smEmpRef = torch.einsum('ni,n,nj->ij', xSamples, 1/normRef, xSamples) / nSamplesRef\n",
    "        smEmpLow = torch.einsum('ni,n,nj->ij', xSamplesLow, 1/normLow, xSamplesLow) / nSamplesLow\n",
    "        # Store second moments in dictionary\n",
    "        smDict['smAnalytic'].append(smAnalytic)\n",
    "        smDict['smEmpRef'].append(smEmpRef)\n",
    "        smDict['smEmpLow'].append(smEmpLow)\n",
    "        ii = ii+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd718c68",
   "metadata": {},
   "source": [
    "Below we plot the individual elements of the covariance\n",
    "matrices, obtained empirically through a large number of\n",
    "samples (X axis), obtained empirically with a small number\n",
    "of samples (red dots), or obtained analytically with the\n",
    "formula above (blue dots).\n",
    "\n",
    "We will see that the empirical estimates of the covariance\n",
    "can be very noisy, but that the analytic expression\n",
    "is practically identical to the large-sample reference empirical\n",
    "distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c4d3cd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### COMPARE ANALYTIC METHOD TO LOW-SAMPLE EMPIRICAL ESTIMATION\n",
    "##############\n",
    "\n",
    "### Plot analytical vs empirical covariances\n",
    "nCols = len(nDim)\n",
    "nRows = len(sigmaVec)\n",
    "\n",
    "ii=0\n",
    "for c in range(nCols):\n",
    "    for r in range(nRows):\n",
    "        # Extract values for this plot\n",
    "        sigma = smDict['sigma'][ii]\n",
    "        df = smDict['nDim'][ii]\n",
    "        smEmpRef = smDict['smEmpRef'][ii].reshape(int(df**2))\n",
    "        smEmpLow = smDict['smEmpLow'][ii].reshape(int(df**2))\n",
    "        smAnalytic = smDict['smAnalytic'][ii].reshape(int(df**2))\n",
    "        ax = plt.subplot(nCols, nRows, ii+1)\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        # scatter the values\n",
    "        plt.scatter(smEmpRef, smEmpLow, c='red', label=f'{nSamplesLow} samples', s=1.5)\n",
    "        plt.scatter(smEmpRef, smAnalytic, color='blue', label='Analytic', s=1.5)\n",
    "        # Add identity line\n",
    "        ax.axline((0,0), slope=1, color='black')\n",
    "        # Add names only on edge panels\n",
    "        if c==(nCols-1):\n",
    "            plt.xlabel('Empirical reference')\n",
    "        if r==0:\n",
    "            plt.ylabel('Estimation')\n",
    "        plt.title(f'd={int(df)}, $\\sigma$ = {sigma:.1f}', fontsize=11)\n",
    "        if c==(nCols-1) and r==(nRows-1):\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "        ii = ii+1\n",
    "# Add color code legend\n",
    "fig = plt.gcf()\n",
    "(lines, labels) = ax.get_legend_handles_labels()\n",
    "lines = lines[0:2]\n",
    "labels = labels[0:2]\n",
    "fig.legend(lines, labels, loc='upper right')\n",
    "fig.set_size_inches(10,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bc893",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### For better visualization of the results, we\n",
    "### view the 3 second moment matrices for one of the cases\n",
    "\n",
    "# Select the noise and dimensions level\n",
    "noiseInd = 3\n",
    "dimInd = 2\n",
    "\n",
    "# Look for the index in the dictionary matching those levels\n",
    "ind = torch.logical_and(smDict['nDim']==nDim[dimInd], \\\n",
    "        smDict['sigma']==sigmaVec[noiseInd])\n",
    "ind = np.flatnonzero(ind)[0]\n",
    "\n",
    "# Plot the matrices\n",
    "plt.title('Second moment matrices')\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(smDict['smAnalytic'][ind])\n",
    "plt.title(f'Analytic', fontsize=11)\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(f'Empirical ref', fontsize=11)\n",
    "plt.imshow(smDict['smEmpRef'][ind])\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(f'Empirical, {int(nSamplesLow)} samples', fontsize=11)\n",
    "plt.imshow(smDict['smEmpLow'][ind])\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(f'Second moments d={nDim[dimInd]}, $\\sigma$={sigmaVec[noiseInd]}')\n",
    "plt.setp(fig.get_axes(), xticks=[], yticks=[])\n",
    "fig.set_size_inches(7,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce101a64",
   "metadata": {},
   "source": [
    "# Non-isotropic Gaussian noise, flexible normalization\n",
    "\n",
    "A more general model for noisy normalized stimuli involves\n",
    "non-isotropic Gaussian noise, $\\gamma \\sim \\mathcal{N}(0, \\Psi)$\n",
    "where $\\Psi$ is any symmetric positive-definite matrix, and\n",
    "flexible normalization, $\\frac{1}{g_{s,f}||s + \\gamma||}$,\n",
    "where $g_{s,f}$ is a normalizing factor that depends on the mean\n",
    "of the stimulus and a linear filter.\n",
    "\n",
    "To estimate the second moment of a noisy, normalized stimulus\n",
    "under these conditions, the following approximation can be used\n",
    "(derived in the companion notes)\n",
    "\n",
    "$$\\mathbb{E}_{\\gamma}\\left( \\frac{XX^T}{g_{s,f}^2||X||^2} \\right) \\approx\n",
    "  \\frac{\\mu_N}{\\mu_D} \\odot \\left( 1 - \\frac{\\Sigma^{N,D}}{\\mu_N\\mu_D} + \\frac{Var(D)}{\\mu_D^2} \\right)$$\n",
    "\n",
    "where $X \\sim \\mathcal{N}(s, \\Psi)$, $\\odot$ indicates matrix-wise\n",
    "multiplication, divisions between matrices are taken element-wise, and\n",
    "$$\\mu_N = \\Psi + s s^T $$\n",
    "$$\\mu_{D} = g_{s,f}^2 (tr(\\Psi) + ||s||^2)$$\n",
    "$$Var(D) = g_{s,f}^4 \\left(2 tr(\\Psi^2) + 4 s^T \\Psi s\\right)$$\n",
    "$$\\Sigma^{N,D} = 2 g_{s,f}^2 \\left[\\Psi \\Psi + s s^T \\Psi +  \\Psi s s^T \\right]$$\n",
    "\n",
    "Note that while $\\mu_N \\in \\mathbb{R}^{d\\times d}$ and\n",
    "$\\Sigma^{N,D} \\in \\mathbb{R}^{d\\times d}$,\n",
    "$\\mu_D \\in \\mathbb{R}$ and $Var(D) \\in \\mathbb{R}$.\n",
    "\n",
    "In the notation above, $N$ indicates moments of the numerator\n",
    "inside the expectation (i.e. products of the form $X_iX_j$)\n",
    "and $D$ refers to moments of the denominator (i.e.\n",
    "$g_{s,f}||X||^2$).\n",
    "\n",
    "Below, we test the performance of this approximation to recover\n",
    "response second moment under these noise and normalization conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91939980",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### DEFINE THE FUNCTION TO COMPUTE THE SECOND MOMENT OF\n",
    "#### ISOTROPIC NOISE, BROADBAND NORM\n",
    "##############\n",
    "def general_flexible_sm(s, sigmaCov, normScale):\n",
    "    \"\"\" Estimate the second moment of a noisy normalized stimulus,\n",
    "    with any Gaussian noise covariance and flexible normalization.\n",
    "    s: Stimulus mean. shape nDim\n",
    "    sigmaCov: Covariance matrix of the noise. shape nDim x nDim\n",
    "    normScale: Scalar that scales normalization.\n",
    "    \"\"\"\n",
    "    # Compute some repeated quantities\n",
    "    ssOuter = torch.einsum('i,j->ij', s, s) # outer prod of stim with itself\n",
    "    sigmaCov2 = torch.matmul(sigmaCov,sigmaCov) # square of covariance matrix\n",
    "    # Mean of numerator\n",
    "    mun = sigmaCov + ssOuter\n",
    "    # Mean of denominator\n",
    "    mud = normScale**2 * (sigmaCov.trace() + torch.einsum('i,i->', s, s))\n",
    "    # Variance of denominator\n",
    "    vard = normScale**4*(2*torch.trace(sigmaCov2) + \\\n",
    "            4*torch.einsum('i,ij,j->', s, sigmaCov, s))\n",
    "    # Correlation between numerator and denominator \n",
    "    covnd = 2 * normScale**2 * (sigmaCov2 + torch.matmul(ssOuter, sigmaCov) + \\\n",
    "        torch.matmul(sigmaCov, ssOuter))\n",
    "    # Expected value of ratio quadratic forms\n",
    "    secondMoment = mun/mud * (1 - covnd/(mun*mud) + vard/(mud**2))\n",
    "    return secondMoment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565d1809",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### COMPUTE ANALYTIC AND EMPIRICAL SECOND MOMENTS\n",
    "##############\n",
    "\n",
    "# All combinations of dimensions and noise below will be used\n",
    "nDim = torch.tensor([10, 50, 200]) # Vector with number of dimensions to use\n",
    "sigmaVec = torch.tensor([0.05, 0.2, 1, 2, 4]) # Vector with sigma values to use to scale cov\n",
    "gVec = torch.tensor([0.7]) # normalizing factor\n",
    "\n",
    "covDiag = False\n",
    "\n",
    "nFits = len(nDim) * len(sigmaVec) * len(gVec)\n",
    "\n",
    "nSamplesRef = torch.tensor(2*10**5) # N Samples for reference empirical distribution\n",
    "nSamplesLow = torch.tensor(1000) # N Samples for noisy empirical estimation\n",
    "\n",
    "smDict = {'nDim': torch.zeros(nFits), 'sigma': torch.zeros(nFits), 'g': torch.zeros(nFits),\n",
    "        'sigmaCov': [], 'smAnalytic': [], 'smEmpRef': [], 'smEmpLow': []}\n",
    "ii=0\n",
    "for d in range(len(nDim)):\n",
    "    for n in range(len(sigmaVec)):\n",
    "        for j in range(len(gVec)):\n",
    "            df = int(nDim[d])  # Dimensions of vector\n",
    "            sigma = sigmaVec[n]   # Standard deviation of noise\n",
    "            g = gVec[j]\n",
    "            # Use a sine function as the mean of the distribution\n",
    "            s = torch.sin(torch.linspace(0, 2*torch.pi, df))\n",
    "            # Store noise and dimension values\n",
    "            smDict['nDim'][ii] = df\n",
    "            smDict['sigma'][ii] = sigma\n",
    "            smDict['g'][ii] = g\n",
    "            # Make a random covariance matrix and scale by sigma\n",
    "            # Make random matrix + cosine to make symmetric matrix\n",
    "            if not covDiag:\n",
    "                #c1 = torch.randn(int(df), int(df)) + \\\n",
    "                #        torch.cos(torch.linspace(0, 3*torch.pi, df)).unsqueeze(1)\n",
    "                #offDiag = torch.matmul(c1, c1.transpose(0,1))\n",
    "                #diagW = torch.rand(1)\n",
    "                #sigmaCov = (offDiag * (1-diagW) + torch.eye(df)*diagW) * sigma\n",
    "                c1 = torch.randn(int(df), int(df))\n",
    "                offDiag = torch.cov(c1)\n",
    "                diagW = torch.rand(1)\n",
    "                sigmaCov = (offDiag * (1-diagW) + torch.eye(df)*diagW) * sigma\n",
    "            else:\n",
    "                sigmaCov = torch.diag((torch.rand(df)+0.5)*2-1)\n",
    "            smDict['sigmaCov'].append(sigmaCov)\n",
    "            ### Calculate analytic second moment:\n",
    "            smAnalytic = general_flexible_sm(s, sigmaCov, g)\n",
    "            ### Calculate empirical reference distribution:\n",
    "            # Initialize distribution of samples\n",
    "            # distribution of x = s + \\gamma\n",
    "            xDist = MultivariateNormal(loc=s, covariance_matrix=sigmaCov)\n",
    "            # Make random samples\n",
    "            xSamples = xDist.rsample([int(nSamplesRef)]) # for reference value\n",
    "            xSamplesLow = xDist.rsample([int(nSamplesLow)]) # for noisy estimation\n",
    "            # Get normalizing factors for the samples\n",
    "            normRef = (torch.norm(xSamples, dim=1)*g)**2\n",
    "            normLow = (torch.norm(xSamplesLow, dim=1)*g)**2\n",
    "            # Compute empirical second moments\n",
    "            smEmpRef = torch.einsum('ni,n,nj->ij', xSamples, 1/normRef, xSamples) / nSamplesRef\n",
    "            smEmpLow = torch.einsum('ni,n,nj->ij', xSamplesLow, 1/normLow, xSamplesLow) / nSamplesLow\n",
    "            # Store second moments in dictionary\n",
    "            smDict['smAnalytic'].append(smAnalytic)\n",
    "            smDict['smEmpRef'].append(smEmpRef)\n",
    "            smDict['smEmpLow'].append(smEmpLow)\n",
    "            ii = ii+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed928f37",
   "metadata": {},
   "source": [
    "Below we compare empirical and analytic results to an\n",
    "empirical reference, like in previous section.\n",
    "\n",
    "We see that the empirical estimates of the covariance\n",
    "can be noisy, and that the analytic expression works\n",
    "well for most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82360991",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### COMPARE ANALYTIC METHOD TO LOW-SAMPLE EMPIRICAL ESTIMATION\n",
    "##############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca6882",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Plot analytical vs empirical covariances\n",
    "nRows = len(nDim)\n",
    "nCols = len(sigmaVec)\n",
    "\n",
    "ii=0\n",
    "for r in range(nRows):\n",
    "    for c in range(nCols):\n",
    "        # Extract values for this plot\n",
    "        sigma = smDict['sigma'][ii]\n",
    "        df = smDict['nDim'][ii]\n",
    "        smEmpRef = smDict['smEmpRef'][ii].reshape(int(df**2))\n",
    "        smEmpLow = smDict['smEmpLow'][ii].reshape(int(df**2))\n",
    "        smAnalytic = smDict['smAnalytic'][ii].reshape(int(df**2))\n",
    "        ax = plt.subplot(nRows, nCols, ii+1)\n",
    "        # scatter the values\n",
    "        plt.scatter(smEmpRef, smEmpLow, c='red', label=f'{nSamplesLow} samples', s=2)\n",
    "        plt.scatter(smEmpRef, smAnalytic, color='blue', label='Analytic', s=2)\n",
    "        # Add identity line\n",
    "        ax.axline((0,0), slope=1, color='black')\n",
    "        # Add names only on edge panels\n",
    "        if r==(nRows-1):\n",
    "            plt.xlabel('Empirical reference')\n",
    "        else:\n",
    "            ax.set_xticks([], [])\n",
    "        if c==0:\n",
    "            plt.ylabel('Estimation')\n",
    "        else:\n",
    "            ax.set_yticks([], [])\n",
    "        plt.title(f'd={int(df)}, $\\sigma$ = {sigma:.2f}', fontsize=11)\n",
    "        if r==(nRows-1) and c==(nCols-1):\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "        ii = ii+1\n",
    "# Add color code legend\n",
    "fig = plt.gcf()\n",
    "(lines, labels) = ax.get_legend_handles_labels()\n",
    "lines = lines[0:2]\n",
    "labels = labels[0:2]\n",
    "fig.legend(lines, labels, loc='upper right')\n",
    "fig.set_size_inches(10,6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e468412a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### For better visualization of the results, we\n",
    "### view the 3 second moment matrices for one of the cases\n",
    "# Select the noise and dimensions level\n",
    "noiseInd = 2\n",
    "dimInd = 2\n",
    "\n",
    "# Look for the index in the dictionary matching those levels\n",
    "ind = torch.logical_and(smDict['nDim']==nDim[dimInd], \\\n",
    "        smDict['sigma']==sigmaVec[noiseInd])\n",
    "ind = np.flatnonzero(ind)[0]\n",
    "\n",
    "# Plot the matrices\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(smDict['smAnalytic'][ind])\n",
    "plt.title(f'Analytic', fontsize=11)\n",
    "plt.subplot(1,3,2)\n",
    "plt.title(f'Empirical ref', fontsize=11)\n",
    "plt.imshow(smDict['smEmpRef'][ind])\n",
    "plt.subplot(1,3,3)\n",
    "plt.title(f'Empirical {int(nSamplesLow)} samples', fontsize=11)\n",
    "plt.imshow(smDict['smEmpLow'][ind])\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(f'Second moments d={int(nDim[dimInd])}, $\\sigma$={sigmaVec[noiseInd]:.2f}')\n",
    "plt.setp(fig.get_axes(), xticks=[], yticks=[])\n",
    "fig.set_size_inches(7,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd01d5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Let's see the random covariance matrices that we sampled for each case\n",
    "ii = 0\n",
    "for r in range(nRows):\n",
    "    for c in range(nCols):\n",
    "        # Extract values for this plot\n",
    "        sigma = smDict['sigma'][ii]\n",
    "        df = smDict['nDim'][ii]\n",
    "        sigmaCov = smDict['sigmaCov'][ii]\n",
    "        ax = plt.subplot(nRows, nCols, ii+1)\n",
    "        ax.set_xticks([], [])\n",
    "        ax.set_yticks([], [])\n",
    "        plt.imshow(sigmaCov)\n",
    "        plt.title(f'd={int(df)}, $\\sigma$ = {sigma:.2f}', fontsize=11)\n",
    "        ii = ii+1\n",
    "fig = plt.gcf()\n",
    "fig.suptitle(f'Noise covariances for each dimension and $\\sigma$')\n",
    "plt.setp(fig.get_axes(), xticks=[], yticks=[])\n",
    "fig.set_size_inches(10,9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695a3e40",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "#########################\n",
    "##### Do empirical vs analytic for the 3D speed dataset\n",
    "#########################\n",
    "#import scipy.io as spio\n",
    "#import numpy as np\n",
    "#import torch\n",
    "#import matplotlib.pyplot as plt\n",
    "#import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "#from torch.utils.data import TensorDataset, DataLoader\n",
    "#\n",
    "## Load ama struct from .mat file into Python\n",
    "#data = spio.loadmat('./data/amaInput_12dir_3speed_0stdDsp_train.mat')\n",
    "## Extract contrast normalized, noisy stimulus\n",
    "#s = data.get(\"Iret\")\n",
    "#s = torch.from_numpy(s)\n",
    "#s = s.transpose(0,1)\n",
    "#s = s.float()\n",
    "## Extract the vector indicating category of each stimulus row\n",
    "#ctgInd = data.get(\"ctgIndMotion\")\n",
    "#ctgInd = torch.tensor(ctgInd)\n",
    "#ctgInd = ctgInd.flatten()\n",
    "#ctgInd = ctgInd-1       # convert to python indexing (subtract 1)\n",
    "#ctgInd = ctgInd.type(torch.LongTensor)  # convert to torch integer\n",
    "## Extract the values of the latent variable\n",
    "#ctgVal = data.get(\"Xmotion\")\n",
    "#ctgVal = torch.from_numpy(ctgVal)\n",
    "#nPixels = int(s.shape[1]/2)\n",
    "## Noise parameters\n",
    "#filterSigma = 0.23\n",
    "#pixelSigma = 0.00075\n",
    "#\n",
    "#def contrast_stim(s):\n",
    "#    nPixels = s.shape[1]\n",
    "#    sMean = torch.mean(s, axis=1)  # Mean intensity of each stimulus, not mean of dataset\n",
    "#    sContrast = torch.einsum('nd,n->nd', (s - sMean.unsqueeze(1)), 1/sMean)\n",
    "#    return sContrast\n",
    "#\n",
    "#s = contrast_stim(s)\n",
    "#\n",
    "#\n",
    "#def analytic_noisy_cov(s, noise=0):\n",
    "#    for \n",
    "#    invNormAn = inv_ncx2(df=df, nc=nc) * (1/sigma**2)\n",
    "#\n",
    "#def analytic_noisy_cov(s, ctgInd):\n",
    "#    nDim = s.shape[1]\n",
    "#    nClasses = ctgInd.unique().shape[0]\n",
    "#    # Compute the conditional statistics of the stimuli\n",
    "#    stimCovs = torch.zeros(nClasses, nDim, nDim)\n",
    "#    stimMeans = torch.zeros(nClasses, nDim)\n",
    "#    for cl in range(nClasses):\n",
    "#        levelInd = [i for i, j in enumerate(ctgInd) if j == cl]\n",
    "#        sLevel = sAll[levelInd, :]\n",
    "#        self.stimCovs[cl, :, :] = torch.cov(sLevel.transpose(0,1))\n",
    "#        self.stimMeans[cl, :] = torch.mean(sLevel, 0)\n",
    "#\n",
    "#"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
