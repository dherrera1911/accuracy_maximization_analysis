{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c1fab05",
   "metadata": {},
   "source": [
    "#Disparity estimation and filter training in pairs\n",
    "\n",
    "Train AMA on the task of disparity estimation. Train two\n",
    "pairs of filters, one after the other (first the model\n",
    "with 2 filters, and then the model with 4 filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb398e07",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### IMPORT PACKAGES\n",
    "##############\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9c1b57",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### COMMENT THIS CELL WHEN USING GOOGLE COLAB\n",
    "#from ama_library import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0e6885",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### UNCOMMENT THIS CELL FOR GOOGLE COLAB EXECUTION\n",
    "!pip install geotorch\n",
    "import geotorch\n",
    "!pip install git+https://github.com/dherrera1911/accuracy_maximization_analysis.git\n",
    "from ama_library import *\n",
    "!mkdir data\n",
    "!wget -O ./data/AMAdataDisparity.mat https://github.com/burgelab/AMA/blob/master/AMAdataDisparity.mat?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daa1304",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### LOAD AMA DATA\n",
    "##############\n",
    "# Load ama struct from .mat file into Python\n",
    "data = spio.loadmat('./data/AMAdataDisparity.mat')\n",
    "# Extract contrast normalized, noisy stimulus\n",
    "s = data.get(\"s\")\n",
    "s = torch.from_numpy(s)\n",
    "s = s.transpose(0,1)\n",
    "s = s.float()\n",
    "# Extract the vector indicating category of each stimulus row\n",
    "ctgInd = data.get(\"ctgInd\")\n",
    "ctgInd = torch.tensor(ctgInd)\n",
    "ctgInd = ctgInd.flatten()\n",
    "ctgInd = ctgInd-1       # convert to python indexing (subtract 1)\n",
    "ctgInd = ctgInd.type(torch.LongTensor)  # convert to torch integer\n",
    "# Extract the values of the latent variable\n",
    "ctgVal = data.get(\"X\")\n",
    "ctgVal = torch.from_numpy(ctgVal)\n",
    "ctgVal = ctgVal.flatten()\n",
    "nPixels = int(s.shape[1]/2)\n",
    "# Extract Matlab trained filters\n",
    "fOri = data.get(\"f\")\n",
    "fOri = torch.from_numpy(fOri)\n",
    "fOri = fOri.transpose(0,1)\n",
    "fOri = fOri.float()\n",
    "# Extract original noise parameters\n",
    "filterSigmaOri = data.get(\"var0\").flatten()\n",
    "maxRespOri = data.get(\"rMax\").flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b8d56",
   "metadata": {},
   "source": [
    "## TEST HOW REPRODUCIBLE THE LEARNED FILTERS ARE, AND TRY DIFFERENT LEARNING PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd39d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### Set the parameters for training the models\n",
    "##############\n",
    "\n",
    "nPairs = 3   # Number of filters to use\n",
    "filterSigma = float(filterSigmaOri / maxRespOri**2)  # Variance of filter responses\n",
    "nEpochs = 50\n",
    "lrGamma = 0.5   # multiplication factor for lr decay\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "learningRate = 0.02\n",
    "lrStepSize = 10\n",
    "batchSize = 1024\n",
    "\n",
    "# Put data into Torch data loader tools\n",
    "trainDataset = TensorDataset(s, ctgInd)\n",
    "# Batch loading and other utilities \n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "# Function that returns an optimizer\n",
    "def opt_fun(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "# Function that returns a scheduler\n",
    "def scheduler_fun(opt):\n",
    "    return torch.optim.lr_scheduler.StepLR(opt, step_size=lrStepSize, gamma=lrGamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d844cdbb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### Train an initial model several times, see filter variability\n",
    "##############\n",
    "\n",
    "nModels = 5\n",
    "loss = [None] * nModels\n",
    "finalLosses = np.zeros((nModels, nPairs))\n",
    "elapsedTimes = [None] * nModels\n",
    "filters = [None] * nModels\n",
    "for n in range(nModels):\n",
    "    amaPy = AMA(sAll=s, nFilt=2, ctgInd=ctgInd, filterSigma=filterSigma,\n",
    "        ctgVal=ctgVal)\n",
    "    loss[n], elapsedTimes[n] = fit_by_pairs(nEpochs=nEpochs, model=amaPy,\n",
    "        trainDataLoader=trainDataLoader, lossFun=lossFun, opt_fun=opt_fun,\n",
    "        nPairs=nPairs, scheduler_fun=scheduler_fun)\n",
    "    filters[n] = amaPy.fixed_and_trainable_filters().detach().clone()\n",
    "    for p in range(nPairs):\n",
    "        finalLosses[n, p] = loss[n][p][-1]\n",
    "\n",
    "# Print the loss of the model after each pair of filters is learned.\n",
    "# Columns indicate the pair of filters, and rows indicate the model instance\n",
    "print(finalLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9d66dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the learned filters\n",
    "nFilt = nPairs * 2\n",
    "for n in range(nModels):\n",
    "    for nf in range(nFilt):\n",
    "        plt.subplot(nModels, nFilt, n*nFilt + nf + 1)\n",
    "        view_filters_bino(filters[n][nf,:])\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71e1db0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### Try out different optimization parameters to see if there's\n",
    "#### differences in the resulting variability\n",
    "##############\n",
    "\n",
    "nModels = 5\n",
    "nPairs = 3   # Numbers of pairs of filters to learn\n",
    "filterSigma = float(filterSigmaOri / maxRespOri**2)  # Variance of filter responses\n",
    "nEpochs = 40\n",
    "lrGamma = 0.5   # multiplication factor for lr decay\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "learningRate = 0.02\n",
    "lrStepSize = 10\n",
    "batchSize = 1024\n",
    "\n",
    "batchSize = [128, 256, 1024]\n",
    "learningRate = [0.04, 0.01]\n",
    "lrGamma = [0.8, 0.5]\n",
    "\n",
    "learnDict = {'batchSize': [], 'learningRate': [], 'lrGamma': [],\n",
    "        'rep': [], 'filters': [], 'finalLosses': []}\n",
    "for bs in range(len(batchSize)):\n",
    "    trainDataLoader = DataLoader(trainDataset, batch_size=batchSize[bs], shuffle=True)\n",
    "    for lr in range(len(learningRate)):\n",
    "        lrDict = bsDict.copy()\n",
    "        def opt_fun(model):\n",
    "            return torch.optim.Adam(model.parameters(), lr=learningRate[lr])\n",
    "        for g in range(len(lrGamma)):\n",
    "            gDict = lrDict.copy()\n",
    "            def scheduler_fun(opt):\n",
    "                return torch.optim.lr_scheduler.StepLR(opt, step_size=lrStepSize, gamma=lrGamma[g])\n",
    "            for n in range(nModels):\n",
    "                nDict = gDict.copy()\n",
    "                amaPy = AMA(sAll=s, nFilt=2, ctgInd=ctgInd, filterSigma=filterSigma,\n",
    "                    ctgVal=ctgVal)\n",
    "                loss, elapsedTimes = fit_by_pairs(nEpochs=nEpochs, model=amaPy,\n",
    "                    trainDataLoader=trainDataLoader, lossFun=lossFun, opt_fun=opt_fun,\n",
    "                    nPairs=nPairs, scheduler_fun=scheduler_fun)\n",
    "                filters = amaPy.fixed_and_trainable_filters().detach().clone()\n",
    "                finalLosses = np.zeros(nPairs)\n",
    "                for p in range(nPairs):\n",
    "                    finalLosses[p] = loss[p][-1]\n",
    "                learnDict['batchSize'].append(batchSize[bs])\n",
    "                learnDict['learningRate'].append(learningRate[lr])\n",
    "                learnDict['lrGamma'].append(lrGamma[g])\n",
    "                learnDict['filters'].append(filters)\n",
    "                learnDict['rep'].append(n)\n",
    "                learnDict['finalLosses'].append(finalLosses)\n",
    "\n",
    "learnDict['batchSize'] = np.array(learnDict['batchSize'])\n",
    "learnDict['learningRate'] = np.array(learnDict['learningRate'])\n",
    "learnDict['lrGamma'] = np.array(learnDict['lrGamma'])\n",
    "learnDict['rep'] = np.array(learnDict['rep'])\n",
    "learnDict['finalLosses'] = np.array(learnDict['finalLosses'])\n",
    "learnDict['filters'] = np.stack(learnDict['filters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4a2dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make scatter plot with losses of the model filters for different\n",
    "# parameters\n",
    "sc = 30\n",
    "plt.scatter(learnDict['batchSize']+np.random.randint(-sc, sc, 60),\n",
    "        learnDict['finalLosses'][:,1],\n",
    "        c=learnDict['learningRate'],\n",
    "        s=learnDict['lrGamma']**2*100)\n",
    "plt.colorbar();\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb716e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the learned filters\n",
    "nFilt = nPairs * 2\n",
    "inds = np.logical_and.reduce((learnDict['batchSize']==256,\n",
    "        learnDict['learningRate']==0.04,\n",
    "        learnDict['lrGamma']==0.5))\n",
    "\n",
    "filters = learnDict['filters'][inds,:,:]\n",
    "for n in range(nModels):\n",
    "    for nf in range(nFilt):\n",
    "        plt.subplot(nModels, nFilt, n*nFilt + nf + 1)\n",
    "        view_filters_bino(filters[n,nf,:])\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
