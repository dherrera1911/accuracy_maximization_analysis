{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea33a3fd",
   "metadata": {},
   "source": [
    "#Disparity estimation and reproducibility of learned filters\n",
    "\n",
    "To understand the properties of the ideal observer models for natural images,\n",
    "it is important to first know how reproducible the learned models are.\n",
    "Here we train the AMA model several times with different seeds and compare\n",
    "the filters learned across runs. The filters are learned in pairs, which\n",
    "seems to improve the reproducibility of the model.\n",
    "\n",
    "We also see the results when using a training procedure that aims to generate\n",
    "more reproducible results. This procedure trains each pair of filters several\n",
    "times using different seeds, and then selects the best of the learned filters\n",
    "at each step. This procedure allows for high reproducibility in the filters\n",
    "learned for the disparity estimation task, as will be shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c102363",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### IMPORT PACKAGES\n",
    "##############\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e80ae4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install geotorch\n",
    "import geotorch\n",
    "!pip install git+https://github.com/dherrera1911/accuracy_maximization_analysis.git\n",
    "from ama_library import *\n",
    "!mkdir data\n",
    "!wget -O ./data/ama_dsp_noiseless.mat https://www.dropbox.com/s/eec1917swc124qd/ama_dsp_noiseless.mat?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c2fbe",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ama_library.ama_class as cl\n",
    "import ama_library.utilities as au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e77e9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### LOAD AMA DATA\n",
    "##############\n",
    "# Load ama struct from .mat file into Python\n",
    "data = spio.loadmat('./data/ama_dsp_noiseless.mat')\n",
    "# Extract contrast normalized, noisy stimulus\n",
    "s = data.get(\"s\")\n",
    "s = torch.from_numpy(s)\n",
    "s = s.transpose(0,1)\n",
    "s = s.float()\n",
    "# Extract the vector indicating category of each stimulus row\n",
    "ctgInd = data.get(\"ctgInd\")\n",
    "ctgInd = torch.tensor(ctgInd)\n",
    "ctgInd = ctgInd.flatten()\n",
    "ctgInd = ctgInd-1       # convert to python indexing (subtract 1)\n",
    "ctgInd = ctgInd.type(torch.LongTensor)  # convert to torch integer\n",
    "# Extract the values of the latent variable\n",
    "ctgVal = data.get(\"X\")\n",
    "ctgVal = torch.from_numpy(ctgVal)\n",
    "ctgVal = ctgVal.flatten().float()\n",
    "nPixels = int(s.shape[1]/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6230405f",
   "metadata": {},
   "source": [
    "## SEE BASELINE FILTER VARIABILITY\n",
    "\n",
    "Train the same model several times using different seeds,\n",
    "and compare the filters learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edecef8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### Set the parameters for training the models\n",
    "##############\n",
    "\n",
    "# Number of models to train with different seeds\n",
    "nModels = 3\n",
    "# Models parameters\n",
    "nPairs = 3   # Number of filters to use\n",
    "pixelNoiseVar = 0.001  # Input pixel noise variance\n",
    "respNoiseVar = 0.003  # Filter response noise variance\n",
    "nEpochs = 20\n",
    "lrGamma = 0.5   # multiplication factor for lr decay\n",
    "lossFun = au.cross_entropy_loss()\n",
    "#lossFun = au.kl_loss()\n",
    "learningRate = 0.01\n",
    "lrStepSize = 5\n",
    "batchSize = 512\n",
    "\n",
    "\n",
    "# Put data into Torch data loader tools\n",
    "trainDataset = TensorDataset(s, ctgInd)\n",
    "# Batch loading and other utilities \n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
    "# Function that returns an optimizer\n",
    "def opt_fun(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "# Function that returns a scheduler\n",
    "def scheduler_fun(opt):\n",
    "    return torch.optim.lr_scheduler.StepLR(opt, step_size=lrStepSize, gamma=lrGamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d8885",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### Train an initial model several times, see filter variability\n",
    "##############\n",
    "loss = [None] * nModels  # List to save the training loss of each model\n",
    "finalLosses = np.zeros((nModels, nPairs))  # Array to save the final loss of each model\n",
    "elapsedTimes = [None] * nModels  # List with the training times of each model\n",
    "filters = [None] * nModels  # List with the filters learned for each model\n",
    "# Loop over the number of models to train\n",
    "for n in range(nModels):\n",
    "    ama = cl.Isotropic(sAll=s, ctgInd=ctgInd, nFilt=2,\n",
    "            respNoiseVar=respNoiseVar, pixelVar=pixelNoiseVar, ctgVal=ctgVal, \n",
    "            filtNorm='broadband', respCovPooling='pre-filter')\n",
    "    loss[n], elapsedTimes[n] = au.fit_by_pairs(nEpochs=nEpochs, model=ama,\n",
    "        trainDataLoader=trainDataLoader, lossFun=lossFun, opt_fun=opt_fun,\n",
    "        nPairs=nPairs, sAll=s, ctgInd=ctgInd, scheduler_fun=scheduler_fun)\n",
    "    filters[n] = ama.fixed_and_trainable_filters().detach().clone()\n",
    "    for p in range(nPairs):\n",
    "        finalLosses[n, p] = loss[n][p][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cfce20",
   "metadata": {},
   "source": [
    "Print the final loss of the model for each pair of learned filters.\n",
    "Each column shows loss for a different filter pair. Each row shows\n",
    "the loss for a different model.\n",
    "\n",
    "We see that there is some variability in the final loss of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35c3d88",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### Print the resulting losses\n",
    "##############\n",
    "# Print the loss of the model after each pair of filters is learned.\n",
    "# Columns indicate the pair of filters, and rows indicate the model instance\n",
    "print(finalLosses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55167834",
   "metadata": {},
   "source": [
    "Show all the filters learned for each model.\n",
    "Each column shows loss for a different filter pair. Each row shows\n",
    "the loss for a different model.\n",
    "\n",
    "We see that there is some variability in the final loss of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8e9d1b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the learned filters\n",
    "nFilt = nPairs * 2\n",
    "fig, axs = plt.subplots(nModels, nFilt)\n",
    "for n in range(nModels):\n",
    "    for nf in range(nFilt):\n",
    "        ax = axs[n, nf]\n",
    "        plt.subplot(nModels, nFilt, n*nFilt + nf + 1)\n",
    "        au.view_1D_bino_image(filters[n][nf,:])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        # Add column label at the top of each column\n",
    "        if n == 0:\n",
    "            ax.set_title(f\"Filter {nf}\")\n",
    "        # Add row label to the left of each row\n",
    "        if nf == 0:\n",
    "            ax.set_ylabel(f\"Model {n}\", rotation=90, ha='center', va='center')\n",
    "fig.tight_layout(rect=(0,0,1,0.95))\n",
    "fig.suptitle(\"Same model parameters, different seeds\", fontsize=14, y=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e047772c",
   "metadata": {},
   "source": [
    "## TRAINING REGIME FOR STABLE MODELS\n",
    "\n",
    "Because it is desirable for our analyses of ideal observers\n",
    "to have reproducible models that consistently converge to the\n",
    "same \"optimal solution\", we implement a training regime to\n",
    "help finding the best model for the task.\n",
    "\n",
    "This training regime involves training each pair of filters\n",
    "several times from different seeds, and choosing the pair of\n",
    "filters that minimize the loss. This procedure should reduce the\n",
    "effects of training randomness on the resulting model. We\n",
    "test the reproducibility of the models trained with this procedure.\n",
    "\n",
    "We will see that with this procedure, the models learned for\n",
    "the disparity estimation task are highly consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3cc4e5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### See filter variability when we train several\n",
    "#### filters at each step and choose the best performing one\n",
    "##############\n",
    "nSeeds = 5  # Number of seeds that are trained for each pair of filters in a model train\n",
    "nModels = 3  # Number of models to train with the procedure, to see stability\n",
    "loss = [None] * nModels\n",
    "finalLosses = np.zeros((nModels, nPairs))\n",
    "elapsedTimes = [None] * nModels\n",
    "filters = [None] * nModels\n",
    "\n",
    "for n in range(nModels):\n",
    "    ama = cl.Isotropic(sAll=s, ctgInd=ctgInd, nFilt=2,\n",
    "            respNoiseVar=respNoiseVar, pixelVar=pixelNoiseVar, ctgVal=ctgVal, \n",
    "            filtNorm='broadband', respCovPooling='pre-filter')\n",
    "    loss[n], elapsedTimes[n] = au.fit_by_pairs(nEpochs=nEpochs, model=ama,\n",
    "        trainDataLoader=trainDataLoader, lossFun=lossFun, opt_fun=opt_fun,\n",
    "        nPairs=nPairs, sAll=s, ctgInd=ctgInd, scheduler_fun=scheduler_fun,\n",
    "        seedsByPair=nSeeds)\n",
    "    filters[n] = ama.fixed_and_trainable_filters().detach().clone()\n",
    "    for p in range(nPairs):\n",
    "        finalLosses[n, p] = loss[n][p][-1]\n",
    "\n",
    "# Print the loss of the model after each pair of filters is learned.\n",
    "# Columns indicate the pair of filters, and rows indicate the model instance\n",
    "print(finalLosses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22395ccb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot filters learned by selecting the best filters at each pair\n",
    "nFilt = nPairs * 2\n",
    "fig, axs = plt.subplots(nModels, nFilt)\n",
    "for n in range(nModels):\n",
    "    for nf in range(nFilt):\n",
    "        ax = axs[n, nf]\n",
    "        plt.subplot(nModels, nFilt, n*nFilt + nf + 1)\n",
    "        au.view_1D_bino_image(filters[n][nf,:])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        # Add column label at the top of each column\n",
    "        if n == 0:\n",
    "            ax.set_title(f\"Filter {nf}\")\n",
    "        # Add row label to the left of each row\n",
    "        if nf == 0:\n",
    "            ax.set_ylabel(f\"Model {n}\", rotation=90, ha='center', va='center')\n",
    "fig.tight_layout(rect=(0,0,1,0.95))\n",
    "fig.suptitle(\"Filters selected from best seed for each model\", fontsize=14, y=0.98)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27f933",
   "metadata": {},
   "source": [
    "## TESTING GROUND FOR TESTING EFFECT OF PARAMETERS ON STABILITY\n",
    "\n",
    "Some model parameters may lead to higher reproducibility, or\n",
    "to good reproducibility at faster speeds. Below several models\n",
    "are trained with different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64df942c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### Try out different optimization parameters to see if there's\n",
    "#### differences in the resulting filter variability\n",
    "##############\n",
    "\n",
    "nModels = 2\n",
    "nPairs = 3   # Numbers of pairs of filters to learn\n",
    "nEpochs = 10\n",
    "lrGamma = 0.5   # multiplication factor for lr decay\n",
    "learningRate = 0.02\n",
    "lrStepSize = 10\n",
    "\n",
    "batchSize = [256, 1024]\n",
    "learningRate = [0.04, 0.01]\n",
    "lrGamma = [0.8, 0.5]\n",
    "\n",
    "learnDict = {'batchSize': [], 'learningRate': [], 'lrGamma': [],\n",
    "        'rep': [], 'filters': [], 'finalLosses': []}\n",
    "for bs in range(len(batchSize)):\n",
    "    trainDataLoader = DataLoader(trainDataset, batch_size=batchSize[bs], shuffle=True)\n",
    "    for lr in range(len(learningRate)):\n",
    "    #    lrDict = bsDict.copy()\n",
    "        def opt_fun(model):\n",
    "            return torch.optim.Adam(model.parameters(), lr=learningRate[lr])\n",
    "        for g in range(len(lrGamma)):\n",
    "    #        gDict = lrDict.copy()\n",
    "            def scheduler_fun(opt):\n",
    "                return torch.optim.lr_scheduler.StepLR(opt, step_size=lrStepSize, gamma=lrGamma[g])\n",
    "            for n in range(nModels):\n",
    "    #            nDict = gDict.copy()\n",
    "                ama = cl.Isotropic(sAll=s, ctgInd=ctgInd, nFilt=2,\n",
    "                        respNoiseVar=respNoiseVar, pixelVar=pixelNoiseVar, ctgVal=ctgVal,\n",
    "                        filtNorm='broadband', respCovPooling='pre-filter')\n",
    "                loss, elapsedTimes = au.fit_by_pairs(nEpochs=nEpochs, model=ama,\n",
    "                    trainDataLoader=trainDataLoader, lossFun=lossFun,\n",
    "                    opt_fun=opt_fun, nPairs=nPairs, sAll=s, ctgInd=ctgInd,\n",
    "                    scheduler_fun=scheduler_fun)\n",
    "                filters = ama.fixed_and_trainable_filters().detach().clone()\n",
    "                finalLosses = np.zeros(nPairs)\n",
    "                for p in range(nPairs):\n",
    "                    finalLosses[p] = loss[p][-1]\n",
    "                learnDict['batchSize'].append(batchSize[bs])\n",
    "                learnDict['learningRate'].append(learningRate[lr])\n",
    "                learnDict['lrGamma'].append(lrGamma[g])\n",
    "                learnDict['filters'].append(filters)\n",
    "                learnDict['rep'].append(n)\n",
    "                learnDict['finalLosses'].append(finalLosses)\n",
    "\n",
    "learnDict['batchSize'] = np.array(learnDict['batchSize'])\n",
    "learnDict['learningRate'] = np.array(learnDict['learningRate'])\n",
    "learnDict['lrGamma'] = np.array(learnDict['lrGamma'])\n",
    "learnDict['rep'] = np.array(learnDict['rep'])\n",
    "learnDict['finalLosses'] = np.array(learnDict['finalLosses'])\n",
    "learnDict['filters'] = np.stack(learnDict['filters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04047e4c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make scatter plot with losses of the model filters for different\n",
    "# parameters\n",
    "sc = 30\n",
    "nConditions = len(learnDict['batchSize'])\n",
    "plt.scatter(learnDict['batchSize'] + np.random.randint(-sc, sc, nConditions),\n",
    "        learnDict['finalLosses'][:,1],\n",
    "        c=learnDict['learningRate'],\n",
    "        s=learnDict['lrGamma']**2*100)\n",
    "plt.xlabel('Batch size')\n",
    "# Add colorbar with title\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('LR')\n",
    "# Create a legend for the size of the dots\n",
    "size_legend_title = 'LR adaptation'\n",
    "sizes =  np.array(lrGamma)*100 # Adjust these values based on your actual data\n",
    "size_labels = [str(size/100) for size in sizes]\n",
    "# Create legend handles\n",
    "legend_handles = [plt.scatter([], [], c='gray', s=size, label=size_label)\n",
    "        for size, size_label in zip(sizes, size_labels)]\n",
    "# Add legend for size\n",
    "plt.legend(handles=legend_handles, title=size_legend_title, loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d2b9f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot filters learned for one combination of parameters\n",
    "nFilt = nPairs * 2\n",
    "inds = np.logical_and.reduce((learnDict['batchSize']==1024,\n",
    "        learnDict['learningRate']==0.04,\n",
    "        learnDict['lrGamma']==0.8))\n",
    "filters = learnDict['filters'][inds,:,:]\n",
    "fig, axs = plt.subplots(nModels, nFilt)\n",
    "for n in range(nModels):\n",
    "    for nf in range(nFilt):\n",
    "        ax = axs[n, nf]\n",
    "        plt.subplot(nModels, nFilt, n*nFilt + nf + 1)\n",
    "        au.view_1D_bino_image(filters[n][nf,:])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "        # Add column label at the top of each column\n",
    "        if n == 0:\n",
    "            ax.set_title(f\"Filter {nf}\")\n",
    "        # Add row label to the left of each row\n",
    "        if nf == 0:\n",
    "            ax.set_ylabel(f\"Model {n}\", rotation=90, ha='center', va='center')\n",
    "fig.tight_layout(rect=(0,0,1,0.95))\n",
    "fig.suptitle(\"Filters selected from best seed for each model\", fontsize=14, y=0.98)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
