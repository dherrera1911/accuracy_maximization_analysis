{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "116a96fa",
   "metadata": {},
   "source": [
    "#Disparity estimation and effect of batch size\n",
    "\n",
    "Train AMA on the task of disparity estimation. Compare\n",
    "the filters learned with different batch sizes, as well\n",
    "as model performance and training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06cad9f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### IMPORT PACKAGES\n",
    "##############\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583a953",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### COMMENT THIS CELL WHEN USING GOOGLE COLAB\n",
    "#from ama_library import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c60ab",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### UNCOMMENT THIS CELL FOR GOOGLE COLAB EXECUTION\n",
    "!pip install geotorch\n",
    "import geotorch\n",
    "!pip install git+https://github.com/dherrera1911/accuracy_maximization_analysis.git\n",
    "from ama_library import *\n",
    "!mkdir data\n",
    "!wget -O ./data/AMAdataDisparity.mat https://github.com/burgelab/AMA/blob/master/AMAdataDisparity.mat?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e002fea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### LOAD AMA DATA\n",
    "##############\n",
    "# Load ama struct from .mat file into Python\n",
    "data = spio.loadmat('./data/AMAdataDisparity.mat')\n",
    "# Extract contrast normalized, noisy stimulus\n",
    "s = data.get(\"s\")\n",
    "s = torch.from_numpy(s)\n",
    "s = s.transpose(0,1)\n",
    "s = s.float()\n",
    "# Extract the vector indicating category of each stimulus row\n",
    "ctgInd = data.get(\"ctgInd\")\n",
    "ctgInd = torch.tensor(ctgInd)\n",
    "ctgInd = ctgInd.flatten()\n",
    "ctgInd = ctgInd-1       # convert to python indexing (subtract 1)\n",
    "ctgInd = ctgInd.type(torch.LongTensor)  # convert to torch integer\n",
    "# Extract the values of the latent variable\n",
    "ctgVal = data.get(\"X\")\n",
    "ctgVal = torch.from_numpy(ctgVal)\n",
    "ctgVal = ctgVal.flatten()\n",
    "nPixels = int(s.shape[1]/2)\n",
    "# Extract Matlab trained filters\n",
    "fOri = data.get(\"f\")\n",
    "fOri = torch.from_numpy(fOri)\n",
    "fOri = fOri.transpose(0,1)\n",
    "# Extract original noise parameters\n",
    "filterSigmaOri = data.get(\"var0\").flatten()\n",
    "maxRespOri = data.get(\"rMax\").flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185fab52",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### SET TRAINING PARAMETERS\n",
    "##############\n",
    "nFilt = 2   # Number of filters to use\n",
    "filterSigma = float(filterSigmaOri / maxRespOri**2)     # Variance of filter responses\n",
    "nEpochsBase = 150\n",
    "lrGamma = 0.3   # multiplication factor for lr decay\n",
    "lossFun = nn.CrossEntropyLoss()\n",
    "learningRate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef6e60a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### FIT AMA WITH DIFFERENT BATCH SIZES\n",
    "##############\n",
    "nStim = s.shape[0]\n",
    "batchFractions = np.array([1/50, 1/20, 1/10, 1/5, 1])\n",
    "nBatchSizes = batchFractions.size\n",
    "batchSizeVec = (nStim*batchFractions).astype(int)\n",
    "\n",
    "filterDict = {\"batchSize\": [], \"filter\": [], \"loss\": [], 'time': [],\n",
    "        'estimates': [], 'estimateStats': []}\n",
    "\n",
    "for bs in range(nBatchSizes):\n",
    "    # Adjust learning parameters to the batch size\n",
    "    nEpochs = int(nEpochsBase*np.sqrt(batchFractions[bs]))\n",
    "    lrStepSize = int(nEpochs/3)\n",
    "    batchSize = int(batchSizeVec[bs])\n",
    "    # Initialize model with random filters\n",
    "    amaPy = AMA(sAll=s, ctgInd=ctgInd, nFilt=nFilt, filterSigma=filterSigma,\n",
    "            ctgVal=ctgVal)\n",
    "    # Put data into Torch data loader tools\n",
    "    trainDataset = TensorDataset(s, ctgInd)\n",
    "    # Batch loading and other utilities \n",
    "    trainDataLoader = DataLoader(trainDataset, batch_size=batchSize,\n",
    "            shuffle=True)\n",
    "    # Set up optimizer\n",
    "    opt = torch.optim.Adam(amaPy.parameters(), lr=learningRate)  # Adam\n",
    "    # Make learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.StepLR(opt, step_size=lrStepSize,\n",
    "            gamma=lrGamma)\n",
    "    #opt = torch.optim.SGD(amaPy.parameters(), lr=0.03)  # SGD\n",
    "    # fit model\n",
    "    loss, elapsedTimes = fit(nEpochs=nEpochs, model=amaPy,\n",
    "            trainDataLoader=trainDataLoader, lossFun=lossFun, opt=opt,\n",
    "            scheduler=scheduler)\n",
    "    # Store ama information into dictionary\n",
    "    filterDict[\"batchSize\"].append(batchSize)\n",
    "    filterDict[\"filter\"].append(amaPy.f.detach())\n",
    "    filterDict[\"loss\"].append(loss)\n",
    "    filterDict[\"time\"].append(elapsedTimes)\n",
    "    filterDict[\"estimates\"].append(amaPy.get_estimates(s))\n",
    "    filterDict[\"estimateStats\"].append(\n",
    "            get_estimate_statistics(filterDict[\"estimates\"][bs], ctgInd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6f3d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DEFINE A FUNCTION TO VISUALIZE BINOCULAR FILTERS\n",
    "def view_filters_bino(f, x=[], title=''):\n",
    "    plt.title(title)\n",
    "    nPixels = int(max(f.shape)/2)\n",
    "    if len(x) == 0:\n",
    "        x = np.arange(nPixels)\n",
    "    plt.plot(x, f[:nPixels], label='L', color='red')\n",
    "    plt.plot(x, f[nPixels:], label='R', color='blue')\n",
    "    plt.ylim(-0.3, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebad63",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the first 2 filters for each batch size\n",
    "x = np.linspace(start=-30, stop=30, num=nPixels) # x axis in arc min\n",
    "for bs in range(nBatchSizes):\n",
    "    for nf in range(nFilt):\n",
    "        plt.subplot(nFilt, nBatchSizes, bs+1+nBatchSizes*nf)\n",
    "        fPlot = filterDict[\"filter\"][bs][nf,:]\n",
    "        view_filters_bino(f=fPlot, x=x, title='size: %i'  %batchSizeVec[bs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf720fbf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the learning curve for each batch size\n",
    "minLoss = 2.75 # Lower limit of y axis\n",
    "maxTime = 5   # Upper limit of X axis in the time plot\n",
    "for bs in range(nBatchSizes):\n",
    "    plt.subplot(2, nBatchSizes, bs+1)\n",
    "    loss = filterDict[\"loss\"][bs]\n",
    "    time = filterDict[\"time\"][bs]\n",
    "    plt.plot(time, np.log(loss))\n",
    "    plt.ylim(np.log(minLoss), np.log(2.98))\n",
    "    plt.xlim(0, maxTime)\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.title('size: %i'  %batchSizeVec[bs])\n",
    "    if bs>0:\n",
    "        plt.yticks([])\n",
    "        plt.ylabel('')\n",
    "    plt.subplot(2, nBatchSizes, bs+1+nBatchSizes)\n",
    "    loss = filterDict[\"loss\"][bs]\n",
    "    epoch = np.arange(loss.size)\n",
    "    plt.plot(epoch, loss)\n",
    "    plt.ylim(minLoss, 2.98)\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    if bs>0:\n",
    "        plt.yticks([])\n",
    "        plt.ylabel('')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
