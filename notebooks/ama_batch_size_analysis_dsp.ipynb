{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38eaa4e",
   "metadata": {},
   "source": [
    "# Disparity estimation and effect of batch size\n",
    "\n",
    "Train AMA on the task of disparity estimation. Compare\n",
    "the filters learned with different batch sizes, as well\n",
    "as model performance and training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527cb9e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### IMPORT PACKAGES\n",
    "##############\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ae5ab0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install geotorch\n",
    "import geotorch\n",
    "!pip install git+https://github.com/dherrera1911/accuracy_maximization_analysis.git\n",
    "!mkdir data\n",
    "!wget -O ./data/ama_dsp_noiseless.mat https://www.dropbox.com/s/eec1917swc124qd/ama_dsp_noiseless.mat?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7d9a0a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ama_library.ama_class as cl\n",
    "import ama_library.utilities as au\n",
    "import ama_library.plotting as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4e103",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### LOAD AMA DATA\n",
    "##############\n",
    "# Load ama struct from .mat file into Python\n",
    "data = spio.loadmat('./data/ama_dsp_noiseless.mat')\n",
    "# Extract contrast normalized, noisy stimulus\n",
    "s = data.get(\"s\")\n",
    "s = torch.from_numpy(s)\n",
    "s = s.transpose(0,1)\n",
    "s = s.float()\n",
    "# Extract the vector indicating category of each stimulus row\n",
    "ctgInd = data.get(\"ctgInd\")\n",
    "ctgInd = torch.tensor(ctgInd)\n",
    "ctgInd = ctgInd.flatten()\n",
    "ctgInd = ctgInd-1       # convert to python indexing (subtract 1)\n",
    "ctgInd = ctgInd.type(torch.LongTensor)  # convert to torch integer\n",
    "# Extract the values of the latent variable\n",
    "ctgVal = data.get(\"X\")\n",
    "ctgVal = torch.from_numpy(ctgVal)\n",
    "ctgVal = ctgVal.flatten().float()\n",
    "nPixels = int(s.shape[1]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbba5f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### SET TRAINING PARAMETERS\n",
    "##############\n",
    "nFilt = 2   # Number of filters to use\n",
    "pixelNoiseVar = 0.001  # Variance of pixel noise\n",
    "respNoiseVar = 0.003  # Variance of filter responses\n",
    "nEpochsBase = 150\n",
    "lrGamma = 0.3   # multiplication factor for lr decay\n",
    "lossFun = au.cross_entropy_loss()\n",
    "learningRate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2467a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### FIT AMA WITH DIFFERENT BATCH SIZES\n",
    "##############\n",
    "nStim = s.shape[0]\n",
    "batchFractions = np.array([1/50, 1/20, 1/10, 1/5, 1])\n",
    "nBatchSizes = batchFractions.size\n",
    "batchSizeVec = (nStim*batchFractions).astype(int)\n",
    "\n",
    "filterDict = {\"batchSize\": [], \"filter\": [], \"loss\": [], 'time': [],\n",
    "        'estimates': [], 'estimateStats': []}\n",
    "\n",
    "for bs in range(nBatchSizes):\n",
    "    # Adjust learning parameters to the batch size\n",
    "    nEpochs = int(nEpochsBase*np.sqrt(batchFractions[bs]))\n",
    "    lrStepSize = int(nEpochs/3)\n",
    "    batchSize = int(batchSizeVec[bs])\n",
    "    # Initialize model with random filters\n",
    "    ama = cl.Isotropic(sAll=s, ctgInd=ctgInd, nFilt=nFilt,\n",
    "            respNoiseVar=respNoiseVar, pixelVar=pixelNoiseVar, ctgVal=ctgVal,\n",
    "            filtNorm='broadband', respCovPooling='pre-filter')\n",
    "    # Put data into Torch data loader tools\n",
    "    trainDataset = TensorDataset(s, ctgInd)\n",
    "    # Batch loading and other utilities \n",
    "    trainDataLoader = DataLoader(trainDataset, batch_size=batchSize,\n",
    "            shuffle=True)\n",
    "    # Set up optimizer\n",
    "    opt = torch.optim.Adam(ama.parameters(), lr=learningRate)  # Adam\n",
    "    # Make learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=lrStepSize,\n",
    "            gamma=lrGamma)\n",
    "    #opt = torch.optim.SGD(ama.parameters(), lr=0.03)  # SGD\n",
    "    # fit model\n",
    "    loss, elapsedTimes = au.fit(nEpochs=nEpochs, model=ama,\n",
    "            trainDataLoader=trainDataLoader, lossFun=lossFun, opt=opt,\n",
    "            sAll=s, ctgInd=ctgInd, scheduler=scheduler)\n",
    "    # Store ama information into dictionary\n",
    "    filterDict[\"batchSize\"].append(batchSize)\n",
    "    filterDict[\"filter\"].append(ama.f.detach())\n",
    "    filterDict[\"loss\"].append(loss)\n",
    "    filterDict[\"time\"].append(elapsedTimes)\n",
    "    filterDict[\"estimates\"].append(ama.get_estimates(s))\n",
    "    filterDict[\"estimateStats\"].append(\n",
    "            au.get_estimate_statistics(filterDict[\"estimates\"][bs], ctgInd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0795c2bd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the first 2 filters for each batch size\n",
    "x = np.linspace(start=-30, stop=30, num=nPixels) # x axis in arc min\n",
    "for bs in range(nBatchSizes):\n",
    "    for nf in range(nFilt):\n",
    "        plt.subplot(nFilt, nBatchSizes, bs+1+nBatchSizes*nf)\n",
    "        fPlot = filterDict[\"filter\"][bs][nf,:]\n",
    "        ap.view_1D_bino_image(inputVec=fPlot, x=x,\n",
    "                title='size: %i'  %batchSizeVec[bs])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d820c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the learning curve for each batch size\n",
    "minLoss = np.min([np.min(arr) for arr in filterDict[\"loss\"]])\n",
    "maxLoss = np.max([np.max(arr) for arr in filterDict[\"loss\"]])\n",
    "margin = (maxLoss - minLoss)*0.05\n",
    "\n",
    "maxTime = 5   # Upper limit of X axis in the time plot\n",
    "for bs in range(nBatchSizes):\n",
    "    plt.subplot(2, nBatchSizes, bs+1)\n",
    "    loss = filterDict[\"loss\"][bs]\n",
    "    time = filterDict[\"time\"][bs]\n",
    "    plt.plot(time, loss)\n",
    "    plt.ylim(minLoss-margin, maxLoss+margin)\n",
    "    plt.xlim(0, maxTime)\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.title('size: %i'  %batchSizeVec[bs])\n",
    "    if bs>0:\n",
    "        plt.yticks([])\n",
    "        plt.ylabel('')\n",
    "    plt.subplot(2, nBatchSizes, bs+1+nBatchSizes)\n",
    "    loss = filterDict[\"loss\"][bs]\n",
    "    epoch = np.arange(loss.size)\n",
    "    plt.plot(epoch, loss)\n",
    "    plt.ylim(minLoss-margin, maxLoss+margin)\n",
    "    plt.ylabel('Cross entropy loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    if bs>0:\n",
    "        plt.yticks([])\n",
    "        plt.ylabel('')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
