{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6426ea0",
   "metadata": {},
   "source": [
    "# Tutorial of the AMA-Gauss Python package\n",
    "\n",
    "Accuracy Maximization Analysis (AMA) is a **dimensionality reduction**\n",
    "technique that learns a set of optimal linear\n",
    "features to solve a **classification task**,\n",
    "given a **Bayesian decoder** of the filter responses. AMA-Gauss\n",
    "is a variant of AMA that assumes Gaussian conditional distributions\n",
    "of the features (we'll refer to AMA-Gauss as AMA throghout).\n",
    "\n",
    "AMA has been used to train image-computable **ideal observer models**\n",
    "for different visual tasks (estimation of retinal speed,\n",
    "disparity, 3D motion, defocus).\n",
    "Unlike other ideal observer models (i.e. models\n",
    "using optimal probabilistic inference to solve a perceptual task),\n",
    "AMA is image-computable. That is,\n",
    "while most ideal observers receive as input a noisy estimate of\n",
    "the latent variable of interest (without specifying how it is\n",
    "estimated from the raw input), AMA receives the raw\n",
    "high-dimensional image and uses it to estimate the latent variable.\n",
    "\n",
    "Unlike other models used to learn optimal sensory\n",
    "encodings of natural image statistics which use efficient\n",
    "coding, or reconstruction error (e.g. sparse coding), AMA learns the\n",
    "optimal encoding to solve a specific sensory tasks.\n",
    "\n",
    "Here we introduce a PyTorch implementation of AMA, trained through\n",
    "gradient descent.\n",
    "We present the mathematical formalism of the model, the different\n",
    "components of AMA class, and the\n",
    "functionalities to train and test an AMA model on a set of stimuli.\n",
    "As an study case, we train AMA on the task of disparity estimation from\n",
    "binocular images.\n",
    "\n",
    "## Basic structure of AMA-Gauss\n",
    "\n",
    "Let $\\mathbf{s}_{i,j} \\in \\mathbb{R}^d$ be an input stimulus\n",
    "(e.g. a binocular image) that is the $i^{th}$ stimulus associated\n",
    "the true value $X_j$ of the latent variable $X$\n",
    "(e.g. the disparity of the image).\n",
    "The latent variable can take values (e.g. a given disparity value in\n",
    "arc min) from a set ${X_1, X_2, ..., X_k}$.\n",
    "The goal of AMA is to compute the\n",
    "posterior probability distribution over $X$, which can be used to read out\n",
    "an estimate of the latent variable for the input image. This will be made\n",
    "clearer below.\n",
    "\n",
    "The AMA-Gauss model consists of 3 stages:\n",
    "\n",
    "1. Noisy encoding of the stimulus and contrast normalization\n",
    "1. Apply noisy linear filters to the noisy normalized stimulus\n",
    "1. Computing the posterior distribution over the latent variable\n",
    "      from the filter responses\n",
    "\n",
    "**1)** Add a sample of white noise\n",
    "$\\gamma \\in \\mathbb{R}^d, \\gamma \\sim \\mathcal{N}\\left(0,\\sigma_s^2 \\right)$\n",
    "to the stimulus, to simulate noisy sensory receptors.\n",
    "Then normalize to unit lenght:\n",
    "\\begin{equation}\n",
    "  \\mathbf{c}_{i,j} = \\frac{\\mathbf{s}_{i,j}+\\mathbf{\\gamma}}{\\lVert\n",
    "      \\mathbf{s}_{i,j}+ \\mathbf{\\gamma} \\rVert}\n",
    "\\end{equation}\n",
    "\n",
    "**2)** Apply a set of noisy linear filters\n",
    "$\\mathbf{f} \\in \\mathbb{R}^{n \\times d}$ to\n",
    "the contrast-normalized stimulus, obtaining a population response vector\n",
    "$\\mathbf{R}_{i,j} \\in \\mathbb{R}^n$:\n",
    "\n",
    "\\begin{equation}\n",
    "  \\mathbf{R}_{i,j} = \\mathbf{f} \\cdot \\mathbf{c}_{i,j} + \\eta\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta \\in \\mathbb{R}^n, \\eta \\sim \\mathcal{N}\\left(0, \\sigma_0^2 \\right)$\n",
    "is a sample of white noise.\n",
    "\n",
    "**3)** Given the filter responses, compute the posterior probabilities\n",
    "of each value of the latent variable, $P(X=X_m|\\mathbf{R}_{i,j})$. For this,\n",
    "compute the likelihood functions \n",
    "$L(X=X_m;\\mathbf{R}_{i,j}) = P(\\mathbf{R}_{i,j}|X_m)$\n",
    "(details will be given below), and combine them with\n",
    "the class priors $P(X_m)$:\n",
    "\n",
    "\\begin{equation}\n",
    "  P(X=X_m|\\mathbf{R}_{i,j}) = L(X=X_m; \\mathbf{R}_{i,j}) P(X=X_m)\n",
    "\\end{equation}\n",
    "\n",
    "In this tutorial, we train an AMA model on a set of binocular images to solve\n",
    "the task of estimating disparity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2c149",
   "metadata": {},
   "source": [
    "## 1) Import and visualize the data\n",
    "\n",
    "We first download and import the binocular images and their\n",
    "disparity values from the [Burge lab](http://burgelab.psych.upenn.edu/) GitHub page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071644a1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### IMPORT PACKAGES\n",
    "##############\n",
    "import scipy.io as spio\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e769b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### IMPORT DISPARITY DATA FROM BURGE LAB GITHUB\n",
    "!mkdir data\n",
    "!wget -O ./data/ama_dsp_noiseless.mat https://www.dropbox.com/s/eec1917swc124qd/ama_dsp_noiseless.mat?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0faf74",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### LOAD DISPARITY DATA\n",
    "##############\n",
    "# Load ama struct from .mat file into Python\n",
    "data = spio.loadmat('./data/ama_dsp_noiseless.mat')\n",
    "# Extract disparity stimuli\n",
    "s = data.get(\"s\")\n",
    "s = torch.from_numpy(s)\n",
    "s = s.transpose(0,1)\n",
    "s = s.float()\n",
    "# Get number of pixels\n",
    "nPixels = int(s.shape[1]/2)\n",
    "# Extract the vector indicating category of each stimulus row\n",
    "ctgInd = data.get(\"ctgInd\")\n",
    "ctgInd = torch.tensor(ctgInd)\n",
    "ctgInd = ctgInd.flatten()\n",
    "ctgInd = ctgInd-1       # convert to python indexing (subtract 1)\n",
    "ctgInd = ctgInd.type(torch.LongTensor)  # convert to torch integer\n",
    "# Extract the values of the latent variable\n",
    "ctgVal = data.get(\"X\")\n",
    "ctgVal = torch.from_numpy(ctgVal)\n",
    "ctgVal = ctgVal.flatten().float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054e67ac",
   "metadata": {},
   "source": [
    "We loaded the binocular images into variable `s`. \n",
    "The stimuli that we loaded are vertically-averaged versions\n",
    "of the original stimuli, and thus are 1D images\n",
    "(see Burge and Geisler JoV 2014 for details).\n",
    "\n",
    "We loaded into variable `ctgInd` a vector containing the index $j$ of the\n",
    "true level $X_j$ of the latent variable $X$ associated with each stimulus.\n",
    "In `ctgVal` we loaded the set of possible levels\n",
    "that $X$ can take.\n",
    "\n",
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75e0d0d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f'Image dataset s has {s.shape[0]} images, of {s.shape[1]} pixels each ({nPixels} pixels per monocular image)')\n",
    "print(f'ctgInd is a vector of length {len(ctgInd)}, with the category index of each s')\n",
    "print(f'ctgVal is a vector of length {len(ctgVal)} containing the possible values of X')\n",
    "print(f'ctgVal ranges between {min(ctgVal)} and {max(ctgVal)} arcmin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e41963",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "#### PLOT RANDOM STIMULUS\n",
    "##############\n",
    "plt.rcParams.update({'font.size': 15})  # increase default font size\n",
    "arcMin = np.linspace(start=-30, stop=30, num=nPixels) # x axis values\n",
    "randomInd = np.random.randint(s.shape[0])  # Select a random stimulus\n",
    "# Get the disparity value\n",
    "stimCategoryInd = ctgInd[randomInd]  # select category index (j) for this stim\n",
    "stimDisparity = ctgVal[stimCategoryInd].numpy()  # Get value of the category (X_j)\n",
    "# Plot the binocular 1D images\n",
    "x = np.linspace(-30, 30, nPixels)\n",
    "plt.plot(x, s[randomInd, :nPixels], label='Left', color='red')  # plot left eye\n",
    "plt.plot(x, s[randomInd, nPixels:], label='Right', color='blue')  #plot right eye\n",
    "plt.ylabel('Weber contrast')\n",
    "plt.xlabel('Visual field (arcmin)')\n",
    "plt.title(f'Stimulus {randomInd}, with j={stimCategoryInd}, $X_j$={stimDisparity} arc min disparity')\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(7,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6c2f1",
   "metadata": {},
   "source": [
    "We saw above the main inputs that the model will receive:\n",
    "the stimuli matrix `s`, their latent variable indices `ctgInd`, and\n",
    "the latent variable values `ctgVal`. Next, we'll see the structure of the\n",
    "AMA-Gauss model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e64d0d",
   "metadata": {},
   "source": [
    "## 2) Download AMA library and initialize AMA object\n",
    "\n",
    "The ama_library (under development) can be found in\n",
    "https://github.com/dherrera1911/accuracy_maximization_analysis.\n",
    "We download and import the library below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378146d7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FIRST WE NEED TO DOWNLOAD AND INSTALL GEOTORCH\n",
    "!pip install geotorch\n",
    "import geotorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf23ac3c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INSTALL THE AMA_LIBRARY PACKAGE FROM GITHUB\n",
    "!pip install git+https://github.com/dherrera1911/accuracy_maximization_analysis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dafef7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# IMPORT AMA LIBRARY\n",
    "##############\n",
    "import ama_library.ama_class as cl\n",
    "import ama_library.utilities as au"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dde88d",
   "metadata": {},
   "source": [
    "In the ama_library, the ama_class module implements the AMA class, which\n",
    "is built on top of the nn.Module from PyTorch. We initialize an AMA\n",
    "object to estimate disparity from binocular image patches.\n",
    "\n",
    "The input parameters to generate the AMA object (mentioned in the\n",
    "AMA formulas above):\n",
    "\n",
    "* Number of filters: $n$ in the equations, `nFilt` input\n",
    "* Pixel noise variance: $\\sigma_s^2$ in the equations, `pixelCov` input\n",
    "* Response noise variance: $\\sigma_0^2$ in the equations, `respNoiseVar` input\n",
    "\n",
    "We need to pass the training dataset as input (the matrix with\n",
    "$\\mathbf{s}$, the vector with associated indexes\n",
    "$j$, and the latent variable values $X_j$).\n",
    "This is because the statistics of the stimulus and of filter responses\n",
    "are stored in the AMA object. Thus, the training dataset is used to\n",
    "estimate the statistics for the initial random filters\n",
    "\n",
    "Let's initialize the AMA model (details about the inputs to AMA\n",
    "initialization can be found in the\n",
    "[AMA GitHub](https://github.com/dherrera1911/accuracy_maximization_analysis/blob/master/ama_library/ama_class.py)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9346f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# INITIALIZE AMA MODEL\n",
    "##############\n",
    "# Set the parameters\n",
    "nFilt = 2  # Create the model with 2 filters\n",
    "pixelNoiseVar = 0.001  # Input pixel noise variance\n",
    "respNoiseVar = 0.003  # Filter response noise variance\n",
    "# Create the untrained AMA object\n",
    "ama = cl.AMA(sAll=s, ctgInd=ctgInd, nFilt=nFilt, respNoiseVar=respNoiseVar,\n",
    "        pixelCov=pixelNoiseVar, ctgVal=ctgVal,\n",
    "        respCovPooling='pre-filter', filtNorm='broadband')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e711b8",
   "metadata": {},
   "source": [
    "Let's list some of the basic attributes of the AMA class.\n",
    "\n",
    "**Attributes:**\n",
    "* `nFilt`: Number of filters in the model ($n$ in the equations)\n",
    "* `nDim`: Number of dimensions in $s$ ($d$ in the equations)\n",
    "* `nClasses`: Number of latent variable levels ($k$ in the equations)\n",
    "* `f`: Filters. Initialized to random variables, these are the trainable parameters, and are constrained to have unit norm. ($\\in \\mathbb{R}^{n \\times d}$).\n",
    "* `ctgVal`: Values of the latent variable ($X_1, X_2, ..., X_k$). ($\\in \\mathbb{R}^{k}$).\n",
    "* `stimCov`: Covariance matrices for the noisy normalized stimuli $\\mathbf{c}$ of each class $X=X_j$. ($\\in \\mathbb{R}^{k \\times d \\times d}$).\n",
    "* `stimMean`: Means for the noisy-normalized stimuli $\\mathbf{c}$ of each class. ($\\in \\mathbb{R}^{k \\times d}$)\n",
    "* `respCov`: Covariance matrices for the noisy responses $\\mathbf{R}_{i,j}$ (including stimulus variability and filter noise). ($\\in \\mathbb{R}^{k \\times n \\times n}$). \n",
    "* `respMean`: Means for the noisy responses $\\mathbf{R}_{i,j}$. ($\\in \\mathbb{R}^{k \\times n}$).\n",
    "\n",
    "Let's verify that the statistics present in the AMA model match\n",
    "what we would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b19b0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# VISUALIZE RANDOMLY INITIALIZED FILTERS\n",
    "##############\n",
    "fInit = ama.f.detach().clone()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x, fInit[0, :nPixels], label='Left', color='red')  # plot left eye\n",
    "plt.plot(x, fInit[0, nPixels:], label='Right', color='blue')  #plot right eye\n",
    "plt.ylabel('Weight')\n",
    "plt.xlabel('Visual field (arcmin)')\n",
    "plt.title(f'Filter 1, random init')\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x, fInit[1, :nPixels], label='Left', color='red')  # plot left eye\n",
    "plt.plot(x, fInit[1, nPixels:], label='Right', color='blue')  #plot right eye\n",
    "plt.xlabel('Visual field (arcmin)')\n",
    "plt.title(f'Filter 2, random init')\n",
    "plt.ylim(-0.4, 0.4)\n",
    "plt.legend()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(11,5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4bdeec",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# COMPUTE EMPIRICAL COVARIANCES OF NOISY NORMALIZED STIMULI\n",
    "##############\n",
    "# Load useful functions\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Extract the stimuli s for the category\n",
    "j = 8  # Index of the category to analyze\n",
    "stimInds = torch.where(ctgInd==j)[0]  # Get indices of the stimuli\n",
    "sj = s[stimInds, ]  # Extract the stimuli of class j\n",
    "\n",
    "# ADD NOISE SAMPLES. FOR BETTER POWER, WE ADD MANY NOISE SAMPLES PER STIMULUS\n",
    "# Make the matrix of pixel covariance noise\n",
    "pixelNoiseCov = torch.eye(sj.shape[1]) * pixelNoiseVar  # Covariance of stim noise\n",
    "# Random noise generator\n",
    "noiseDistr = MultivariateNormal(loc=torch.zeros(sj.shape[1]),\n",
    "        covariance_matrix=pixelNoiseCov)\n",
    "# Number of noisy samples to generate for each stimulus\n",
    "samplesPerStim = 1000\n",
    "# Repeat class stimuli to use many noise samples\n",
    "repStim = sj.repeat(samplesPerStim, 1)\n",
    "# Add noise\n",
    "noisyStim = repStim + noiseDistr.rsample([repStim.shape[0]])\n",
    "# Normalize to unit norm\n",
    "noisyNormStim = F.normalize(noisyStim, p=2, dim=1)\n",
    "# Apply the initialized filters\n",
    "responses = torch.matmul(noisyNormStim, fInit.transpose(0, 1))\n",
    "# COMPUTE THE COVARIANCES OF NOISY NORMALIZED STIMULI\n",
    "stimCovEmpirical = torch.cov(noisyNormStim.transpose(0,1))\n",
    "respCovEmpiricalNoiseless = torch.cov(responses.transpose(0,1))\n",
    "# Add response noise\n",
    "respCovEmpirical = respCovEmpiricalNoiseless + torch.eye(nFilt) * respNoiseVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37a91e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# COMPARE EMPIRICAL COVARIANCES AND AMA-ESTIMATED COVARIANCES\n",
    "##############\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(stimCovEmpirical)\n",
    "plt.title('Cov(s) [empirical]')\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(ama.stimCov[j,:,:].detach())\n",
    "plt.title('Cov(s) [AMA]')\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(respCovEmpirical)\n",
    "plt.title('Cov(R) [empirical]')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(ama.respCov[j,:,:].detach())\n",
    "plt.title('Cov(R) [AMA]')\n",
    "fig = plt.gcf()\n",
    "fig.suptitle('Stimulus and response covariance for one class. Empirical vs AMA')\n",
    "plt.setp(fig.get_axes(), xticks=[], yticks=[])\n",
    "fig.set_size_inches(10,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1990bdba",
   "metadata": {},
   "source": [
    "## 3) Decoding in AMA model\n",
    "\n",
    "Now that we layed out the AMA model structure, we next show the decoding\n",
    "of the latent class using the AMA object.\n",
    "\n",
    "To decode the latent variable class, we assume that the noisy filter\n",
    "responses are Gaussian distributed, conditional on the latent variable\n",
    "class. Thus, for each class $j$ we have the mean of the filter\n",
    "responses to the class stimuli, $\\mathbf{mu}_j \\in \\mathbb{R}^{d}$,\n",
    "and the covariance of the responses\n",
    "$\\mathbf{\\Psi}_j \\in \\mathbb{R}^{d \\times d}$.\n",
    "\n",
    "Thus, the probability of observing the response vector $\\mathbf{R}_i$\n",
    "if $X = X_j$ is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\mathbf{R}_i | X=X_j) = \\frac{1}{\\sqrt{(2\\pi)^n |\\mathbf{\\Psi_j}|}}\n",
    "    \\exp\\left( -\\frac{1}{2} (\\mathbf{R}_i-\\boldsymbol{\\mu}_j)^T\n",
    "    \\mathbf{\\Psi}_j^{-1} (\\mathbf{R}_i-\\boldsymbol{\\mu}_j) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "As shown above, each $\\mathbf{\\Psi}_j$ and $\\boldsymbol{\\mu}$ was computed\n",
    "for the initial random filters. The attribute `ama.respCov[j,:,:]` contains\n",
    "$\\mathbf{\\Psi}_j$ and the attribute `ama.respMean[j,:]` contains\n",
    "$\\boldsymbol{\\mu}_j$.\n",
    "\n",
    "Assuming a flat prior $P(X=X_j) = \\frac{1}{k}$, the posterior\n",
    "distribution over the latent variable given a filter population\n",
    "response $\\mathbf{R}_i$ is then given by the following formula:\n",
    "\n",
    "\\begin{equation}\n",
    "  P(X=X_j | \\mathbf{R}_i) = \\frac{P(\\mathbf{R}_i | X=X_j)}{\\sum_{i=1}^{i=k}\n",
    "          P(\\mathbf{R}_i | X=X_i)}\n",
    "\\end{equation}\n",
    "\n",
    "We can then use the posterior distribution to decode a value of $X$,\n",
    "for example by choosing the *Minimum Mean Square Estimate* (MMSE) or the\n",
    "*Maximum A Posteriori* estimate. This completes the probabilistic\n",
    "ideal-observer model that takes stimuli as inputs, and generates\n",
    "stimulus estimates as outputs.\n",
    "\n",
    "We next show how to carry out this process with the AMA object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fda7d1",
   "metadata": {},
   "source": [
    "### Visualize response statistics\n",
    "\n",
    "Let's first see the distribution of responses in the dataset.\n",
    "We will use a function included in the AMA object that computes\n",
    "the responses to a set of input stimuli, and plotting functions\n",
    "from the `utility` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ceb6a4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# PLOT DISTRIBUTION OF NOISY FILTER RESPONSES\n",
    "##############\n",
    "# Select a subset of categories to visualize\n",
    "ctgVis = torch.arange(start=1, end=ctgInd.max(), step=4)\n",
    "# Extract the stimuli corresponding to these categories\n",
    "visInds = torch.where(torch.isin(ctgInd, ctgVis))[0]\n",
    "sVis = s[visInds, :]\n",
    "ctgIndVis = ctgInd[visInds]\n",
    "# Obtain the noisy responses to these stimuli\n",
    "respVis = ama.get_responses(s=sVis, addStimNoise=True, addRespNoise=True)\n",
    "respVis = respVis.detach()\n",
    "# Plot responses and the ama-estimated ellipses\n",
    "fig, ax = plt.subplots()\n",
    "au.view_response_ellipses(resp=respVis, covariance=ama.respCov.detach(),\n",
    "        ctgInd=ctgIndVis, ctgVal=ctgVal, plotFilt=torch.tensor([0,1]),\n",
    "        fig=fig, ax=ax)\n",
    "cax = plt.gca().collections[-1].colorbar\n",
    "cax.set_label('Disparity')\n",
    "fig.set_size_inches(10,8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec19de",
   "metadata": {},
   "source": [
    "### Visualize stimulus posteriors\n",
    "\n",
    "The latent-variable conditional Gaussian distributions shown above\n",
    "as ellipses are used for decoding the latent variable from filter\n",
    "responses. Below, we show the posterior distributions obtained\n",
    "for the responses to each stimulus of a given category, using the set\n",
    "of distributions shown above. Then, we show the mean estimate\n",
    "for the all the stimuli across each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b96fd35",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# VISUALIZE POSTERIOR DISTRIBUTIONS\n",
    "##############\n",
    "j = 6  # category to visualize\n",
    "jInd = torch.where(ctgInd == j)[0]  # Select stimuli indices\n",
    "ctgPosteriors = ama.get_posteriors(s=s[jInd,:],\n",
    "        addStimNoise=True, addRespNoise=True)  # Compute posteriors\n",
    "ctgPosteriors = ctgPosteriors.detach()  # detach from pytorch gradient\n",
    "# Plot the posteriors\n",
    "plt.plot(ctgVal, ctgPosteriors.transpose(0,1), color='black', linewidth=0.2)\n",
    "plt.axvline(x=ctgVal[j], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fadc0c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# VISUALIZE THE mean estimates of the model for each model category\n",
    "# CATEGORY\n",
    "##############\n",
    "# Get the estimates for each stimulus using the current untrained filters\n",
    "estimUntr = ama.get_estimates(s=s, method4est='MAP', addStimNoise=True,\n",
    "        addRespNoise=True)\n",
    "# Summarize the estimates into means and SD for each class\n",
    "estimSummUntr = au.get_estimate_statistics(estimates=estimUntr,\n",
    "        ctgInd=ctgInd)\n",
    "# Plot the mean estimates for each class\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ctgVal, estimSummUntr['estimateMean'])\n",
    "plt.fill_between(ctgVal, estimSummUntr['lowCI'], estimSummUntr['highCI'],\n",
    "        color='blue', alpha=0.2, label='95% CI')\n",
    "ax.axline((0, 0), slope=1, color='black')\n",
    "plt.ylim(ctgVal.min(), ctgVal.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7dbda7",
   "metadata": {},
   "source": [
    "## 4) Training the model\n",
    "\n",
    "So far, we showed how the AMA object takes a stimulus and\n",
    "generates a posterior probability distribution over latent-variable\n",
    "values. But the model was initialized with random filters, and\n",
    "so the behavior of the model is not very useful or informative.\n",
    "\n",
    "However, all the operations discussed above to get from the stimuli\n",
    "to the posteriors are differentiable with respect to the filters.\n",
    "This means that we can define a loss function, and use standard\n",
    "Pytorch tools to perform gradient descent on the filter to\n",
    "minimize the loss. Below, we show the procedure to train the model,\n",
    "using functions defined in ama.utilities. We then show the\n",
    "results of training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218061f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# DEFINE MODEL TRAINING PARAMETERS\n",
    "##############\n",
    "nEpochs = 50\n",
    "lrGamma = 0.3   # multiplication factor for lr decay\n",
    "lrStepSize = nEpochs/3\n",
    "learningRate = 0.01\n",
    "batchSize = 1024\n",
    "#lossFun = au.nll_loss()  # Negative-log-likelihood loss (uses n-log-likelihood)\n",
    "lossFun = au.cross_entropy_loss()  # Cross-entropy loss (uses posteriors)\n",
    "\n",
    "##############\n",
    "# GENERATE OBJECTS REQUIRED FOR TRAINING\n",
    "##############\n",
    "# Put data into Torch data loader tools\n",
    "trainDataset = TensorDataset(s, ctgInd)\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=batchSize,\n",
    "    shuffle=True)\n",
    "# Set up optimizer\n",
    "opt = torch.optim.Adam(ama.parameters(), lr=learningRate)  # Adam\n",
    "# Set up scheduler to adapt learning rate\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=lrStepSize,\n",
    "        gamma=lrGamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2b26e9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# TRAIN THE MODEL\n",
    "##############\n",
    "loss, time = au.fit(nEpochs=nEpochs, model=ama,\n",
    "        trainDataLoader=trainDataLoader, lossFun=lossFun,\n",
    "        opt=opt, sAll=s, ctgInd=ctgInd, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26fb3f",
   "metadata": {},
   "source": [
    "### Visualize trained model\n",
    "\n",
    "Now the model is trained, let's look at what changed. The functions\n",
    "below also show how to use the trained model (e.g. to get predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be105ad6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# PLOT LOSS THROUGH TRAINING\n",
    "##############\n",
    "plt.plot(loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fcaabc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# PLOT RESULTING FILTERS\n",
    "##############\n",
    "au.view_all_filters_bino(ama)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9086b6d",
   "metadata": {},
   "source": [
    "We see above that the filters are learned very quickly,\n",
    "mostly in only a few epochs. Also, the resulting filters\n",
    "look reasonable, forming a pair of filters each of which is\n",
    "smooth, symmetric across the two eyes, and symmetric with\n",
    "the other filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e1e6e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# PLOT RESPONSE DISTRIBUTION WITH TRAINED MODEL\n",
    "##############\n",
    "# Select a subset of categories to visualize\n",
    "ctgVis = torch.arange(start=1, end=ctgInd.max(), step=3)\n",
    "# Extract the stimuli corresponding to these categories\n",
    "visInds = torch.where(torch.isin(ctgInd, ctgVis))[0]\n",
    "sVis = s[visInds, :]\n",
    "ctgIndVis = ctgInd[visInds]\n",
    "# Obtain the noisy responses to these stimuli\n",
    "respVis = ama.get_responses(s=sVis, addStimNoise=True, addRespNoise=True)\n",
    "respVis = respVis.detach()\n",
    "# Plot response distribution and Gaussian fit ellipses\n",
    "au.view_response_ellipses(resp=respVis, covariance=ama.respCov.detach(),\n",
    "        ctgInd=ctgIndVis, ctgVal=ctgVal, plotFilt=torch.tensor([0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e43b997",
   "metadata": {},
   "source": [
    "Above we also see that the distribution of filter responses seems\n",
    "well approximated by the Gaussian ellipses, indicating that our\n",
    "assumption of conditional Gaussian distribution of filter responses\n",
    "$\\mathbf{R}$ is supported in this case. Also, as could be expected from\n",
    "the properties of the filters and the images (both with 0 mean, and\n",
    "the images approximately translation invariant), the mean filter\n",
    "responses are approximately 0 for each class, and classes are mostly\n",
    "separated by their second-order statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a337fa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# PLOT THE ESTIMATES MEAN FOR EACH CATEGORY\n",
    "##############\n",
    "# Get estimates with the new trained model\n",
    "estimTrained = ama.get_estimates(s=s, method4est='MAP', addStimNoise=True,\n",
    "        addRespNoise=True)\n",
    "estimTrained = estimTrained.detach()\n",
    "estimSummTrained = au.get_estimate_statistics(estimates=estimTrained,\n",
    "        ctgInd=ctgInd)\n",
    "# Plot the estimates means and CIs\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ctgVal, estimSummTrained['estimateMean'])\n",
    "plt.fill_between(ctgVal, estimSummUntr['lowCI'], estimSummUntr['highCI'],\n",
    "        color='blue', alpha=0.2, label='95% CI')\n",
    "ax.axline((0, 0), slope=1, color='black')\n",
    "plt.ylim(ctgVal.min(), ctgVal.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eef73f2",
   "metadata": {},
   "source": [
    "Above we finally see that the separation of response distributions\n",
    "obtained after model training translated into an improvement in model\n",
    "performance, although model performance is still far from optimal with\n",
    "only 2 filters."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
