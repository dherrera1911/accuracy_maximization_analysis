import numpy as np
import torch
from torch import optim
from torch import fft as fft
import torch.nn.functional as F
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from ama_library import quadratic_moments as qm
import time

##################################
##################################
#
## FUNCTIONS FOR FITTING AMA MODELS
#
##################################
##################################
#
# This group of functions take an ama model, and some inputs
# such as the loss function, and do the training loop.
# Different types of training are available, such as training
# the filters in pairs, or with multiple seeds


# Define loop function to train the model
def fit(nEpochs, model, trainDataLoader, lossFun, opt, sAll,
        ctgInd, scheduler=None, addStimNoise=True,
        addRespNoise=True):
    """ Fit AMA model using the posterior distribuions generated by the model.
    nEpochs: Number of epochs. Integer
    model: AMA model object.
    trainDataLoader: data loader generated with torch.utils.data.DataLoader
    lossFun: loss function that uses posterior distribution over classes.
    opt: optimizer, selected from torch.optim
    scheduler: scheduler for adaptive learning rate, generated with optim.lr_scheduler
    """
    trainingLoss = np.zeros(nEpochs+1)
    elapsedTime = np.zeros(nEpochs+1)
    # Get the loss of the full dataset stored in the data loader
    trainingLoss[0] = lossFun(model, trainDataLoader.dataset.tensors[0],
            trainDataLoader.dataset.tensors[1]).detach()
    print('Initial loss: ', trainingLoss[0])
    opt.zero_grad()
    # If narrowband, precompute amplitude spectrum for more speed
    if model.filtNorm == 'narrowband':
        sAmp = qm.compute_amplitude_spectrum(s=sAll)
    else:
        sAmp = None
    # Measure time and start loop
    start = time.time()
    for epoch in range(nEpochs):
        for sb, ctgb in trainDataLoader:
            # Generate predictions for batch sb, returned by trainDataLoader 
            model.update_response_statistics(sAll=sAll, ctgInd=ctgInd,
                    sAmp=sAmp)
            loss = lossFun(model, sb, ctgb)     # Compute loss
            loss.backward()                     # Compute gradient
            opt.step()                          # Take one step
            opt.zero_grad()                     # Restart gradient
        # Print model loss
        trainingLoss[epoch+1] = lossFun(model, trainDataLoader.dataset.tensors[0],
                trainDataLoader.dataset.tensors[1]).detach()
        trainingDiff = trainingLoss[epoch+1] - trainingLoss[epoch]
        print('Epoch: %d |  Training loss: %.4f  |  Loss change: %.4f' %
                (epoch+1, trainingLoss[epoch+1], trainingDiff))
        end = time.time()
        elapsedTime[epoch+1] = end - start
        # Apply scheduler step
        if not scheduler == None:
            if "ReduceLROnPlateau" in str(type(scheduler)):
                scheduler.step(trainingLoss[epoch+1])    # adapt learning rate
            else:
                scheduler.step()
    # Do the final response statistics update
    model.update_response_statistics(sAll=sAll, ctgInd=ctgInd, sAmp=sAmp)
    return trainingLoss, elapsedTime


# LOOP TO TRAIN MULTIPLE SEEDS AND CHOOSE BEST
def fit_multiple_seeds(nEpochs, model, trainDataLoader, lossFun, opt_fun,
        scheduler_fun=None, nSeeds=1):
    """ Fit AMA model multiple times from different seeds, and keep the result with
    best performance.
    nEpochs: Number of epochs for each pair of filters. Integer
    model: AMA model object.
    trainDataLoader: data loader generated with torch.utils.data.DataLoader
    lossFun: loss function that uses posterior distribution over classes.
    opt_fun: A function that takes in a model and returns an optimizer
    scheduler_fun: Function that takes in an optimizer and returns a
        scheduler for that optimizer.
    nSeeds: number of times to train the filters among which to choose
        the best ones
    """
    if (nSeeds>1):
        # Initialize lists to fill
        seedLoss = np.zeros(nSeeds)
        trainingLoss = [None] * nSeeds
        elapsedTimes = [None] * nSeeds
        filters = [None] * nSeeds
        for p in range(nSeeds):
            if (p>0):
                model.reinitialize_trainable()
            # Train the current pair of trainable filters
            opt = opt_fun(model)
            if (scheduler_fun == None):
                scheduler = None
            else:
                scheduler = scheduler_fun(opt)
            # Train random initialization of the model
            trainingLoss[p], elapsedTimes[p] = fit(nEpochs=nEpochs, model=model,
                    trainDataLoader=trainDataLoader, lossFun=lossFun, opt=opt,
                    scheduler=scheduler)
            filters[p] = model.f.detach().clone()
            # Get the final loss of the filters of this seetrainingLossd
            seedLoss[p] = trainingLoss[p][-1]
        # Set the filter with the minimum loss into the model
        minFilt = seedLoss.argmin()
        model.assign_filter_values(filters[minFilt])
        # Return only the training loss history of the best filter
        minLoss = trainingLoss[minFilt]
        minElapsed = elapsedTimes[minFilt]
    else:
        opt = opt_fun(model)
        if (scheduler_fun == None):
            scheduler = None
        else:
            scheduler = scheduler_fun(opt)
        minLoss, minElapsed = fit(nEpochs=nEpochs, model=model,
                trainDataLoader=trainDataLoader, lossFun=lossFun, opt=opt,
                scheduler=scheduler)
    return minLoss, minElapsed


# TRAIN MODEL FILTERS IN PAIRS, WITH POSSIBLE SEED SELECTION
def fit_by_pairs(nEpochs, model, trainDataLoader, lossFun, opt_fun,
        nPairs, scheduler_fun=None, seedsByPair=1):
    """ Fit AMA model training filters by pairs. After a pair is trained, it
    is fixed in place (no longer trainable), and a new set of trainable
    filters is then initialized and trained. Has the option to try different
    seeds for each pair of filters trained, and choosing the best pair
    nEpochs: Number of epochs for each pair of filters. Integer
    model: AMA model object.
    trainDataLoader: data loader generated with torch.utils.data.DataLoader
    lossFun: loss function that uses posterior distribution over classes.
    opt_fun: A function that takes in a model and returns an optimizer
    nPairs: Number of pairs to train. nPairs=1 corresponds to only training
        the filters included in the input model.
    scheduler_fun: Function that takes in an optimizer and returns a
        scheduler for that optimizer.
    seedsByPair: number of times to train each pair from different random
        initializations, to choose the best pair.
    """
    trainingLoss = [None] * nPairs
    elapsedTimes = [None] * nPairs
    # Measure time and start loop
    start = time.time()
    for p in range(nPairs):
        # If not the first iteration, fix current filters and add new trainable
        if (p>0):
            fAll = model.fixed_and_trainable_filters().detach().clone()
            model.add_fixed_filters(fAll)
            model.reinitialize_trainable()
        print(f'Pair {p}')
        # Train the current pair of trainable filters
        trainingLoss[p], elapsedTimes[p] = fit_multiple_seeds(nEpochs=nEpochs,
                model=model, trainDataLoader=trainDataLoader, lossFun=lossFun,
                opt_fun=opt_fun, scheduler_fun=scheduler_fun, nSeeds=seedsByPair)
        end = time.time()
        elapsedTime = end - start
        print(f'########## Pair {p+1} trained in {elapsedTime} ##########')
    # Put all the filters into the f model attribute
    fAll = model.fixed_and_trainable_filters().detach().clone()
    model.assign_filter_values(fAll)
    model.add_fixed_filters(torch.tensor([]))
    return trainingLoss, elapsedTimes


##################################
##################################
#
## LOSS FUNCTIONS
#
##################################
##################################
#
# Define loss functions that take as input AMA model, so
# different outputs can be used with the same fitting functions


def cross_entropy_loss():
    """ Cross entropy loss for AMA.
    model: AMA model object
    s: input stimuli. tensor shaped batch x features
    ctgInd: true categories of stimuli, as a vector with category index
        type torch.LongTensor"""
    crossEnt = torch.nn.CrossEntropyLoss()
    def lossFun(model, s, ctgInd):
        loss = crossEnt(model.get_posteriors(s, addStimNoise=F), ctgInd)
        return loss
    return lossFun


def nll_loss():
    """ Negative log-likelihood loss for AMA.
    model: AMA model object
    s: input stimuli. tensor shaped batch x features
    ctgInd: true categories of stimuli, as a vector with category index
        type torch.LongTensor"""
    crossEnt = torch.nn.CrossEntropyLoss()
    def lossFun(model, s, ctgInd):
        logProbs = F.log_softmax(model.get_log_likelihood(s), dim=1)
        loss = crossEnt(logProbs, ctgInd)
        return loss
    return lossFun


def mse_loss():
    """ MSE loss for AMA. Computes MSE between the latent variable
    estimate
    model: AMA model object
    s: input stimuli. tensor shaped batch x features
    ctgInd: true categories of stimuli. type torch.LongTensor"""
    mseLoss = torch.nn.MSELoss()
    def lossFun(model, s, ctgInd):
        loss = mseLoss(model.get_estimates(s, method4est='MMSE'),
                model.ctgVal[ctgInd])
        return loss
    return lossFun


def mae_loss():
    """ MAE loss for AMA. Computes MAE between the latent variable
    estimate 
    model: AMA model object
    s: input stimuli. tensor shaped batch x features
    ctgInd: true categories of stimuli. type torch.LongTensor"""
    mseLoss = torch.nn.L1Loss()
    def lossFun(model, s, ctgInd):
        loss = mseLoss(model.get_estimates(s, method4est='MMSE'),
                model.ctgVal[ctgInd])
        return loss
    return lossFun


##################################
##################################
#
## Stimulus processing
#
##################################
##################################
#
# Define loss functions that take as input AMA model, so
# different outputs can be used with the same fitting functions


#
## STIMULUS PROCESSING
#

def contrast_stim(s):
    """Take a batch of stimuli and convert to Weber contrast stimulus
    That is, subtracts the stimulus mean, and then divides by the mean.
    s: Stimuli batch. nStim x nDimensios"""
    # Mean intensity of each stimulus, not mean of dataset
    sMean = torch.mean(s, axis=1)
    sContrast = torch.einsum('nd,n->nd', (s - sMean.unsqueeze(1)), 1/sMean)
    return sContrast


#
## FUNCTIONS FOR SUMMARY AND EVALUATION OF MODEL PERFORMANCE
#

# Function that turns posteriors into estimate averages, SDs and CIs
def get_estimate_statistics(estimates, ctgInd, quantiles=[0.025, 0.975]):
    # Compute means and stds for each true level of the latent variable
    estimatesMeans = torch.zeros(ctgInd.max()+1)
    estimatesSD = torch.zeros(ctgInd.max()+1)
    lowCI = torch.zeros(ctgInd.max()+1)
    highCI = torch.zeros(ctgInd.max()+1)
    quantiles = torch.tensor(quantiles)
    for cl in ctgInd.unique():
        levelInd = [i for i, j in enumerate(ctgInd) if j == cl]
        estimatesMeans[cl] = estimates[levelInd].mean()
        estimatesSD[cl] = estimates[levelInd].std()
        (lowCI[cl], highCI[cl]) = torch.quantile(estimates[levelInd], quantiles)
    return {'estimateMean': estimatesMeans, 'estimateSD': estimatesSD,
            'lowCI': lowCI, 'highCI': highCI}


## FUNCTIONS FOR PLOTTING FILTERS OF AMA MODEL
# Functions for plotting the filters
def unvectorize_filter(fIn, frames=15, pixels=30):
    nFilt = fIn.shape[0]
    matFilt = fIn.reshape(nFilt, 2, frames, pixels)
    matFilt = matFilt.transpose(1,2).reshape(nFilt, frames, pixels*2)
    return matFilt


# Plot filters
def view_filters_bino_video(fIn, frames=15, pixels=30):
    matFilt = unvectorize_filter(fIn, frames=frames, pixels=pixels)
    nFilters = matFilt.shape[0]
    for k in range(nFilters):
        plt.subplot(nFilters, 1, k+1)
        plt.imshow(matFilt[k,:,:].squeeze(), cmap='gray')
        ax = plt.gca()
        ax.axes.xaxis.set_visible(False)
        ax.axes.yaxis.set_visible(False)
    return ax

# DEFINE A FUNCTION TO VISUALIZE BINOCULAR FILTERS
def view_filters_bino(f, x=[], title=''):
    plt.title(title)
    nPixels = int(max(f.shape)/2)
    if len(x) == 0:
        x = np.arange(nPixels)
    plt.plot(x, f[:nPixels], label='L', color='red')
    plt.plot(x, f[nPixels:], label='R', color='blue')
    plt.ylim(-0.3, 0.3)

# DEFINE A FUNCTION TO VISUALIZE ALL BINOCULAR FILTERS OF AN AMA MODEL
def view_all_filters_bino(amaPy, x=[]):
    fAll = amaPy.fixed_and_trainable_filters()
    fAll = fAll.detach()
    nFiltAll = fAll.shape[0]
    nPairs = int(nFiltAll/2)
    for n in range(nFiltAll):
        plt.subplot(nPairs, 2, n+1)
        view_filters_bino(fAll[n,:], x=[], title=f'F{n}')


def subsample_covariance(covariance, classInd, filtInd):
    """ Takes a tensor of shape k x d x d holding the covariance
    matrices for k classes and d filters, and returns a smaller
    tensor with the covariances matrices of the classes given in
    classInd, and of the filters in filtInd.
    Eg. if classInd=[2, 3, 4] and filtInd=[0,3], it returns the
    covariance matrix between filters 0 and 3, for classes 2,3,4."""
    covPlt = covariance[classInd, :, :]
    covPlt = covPlt[:, filtInd, :]
    covPlt = covPlt[:, :, filtInd]
    return covPlt


def view_response_ellipses(resp, covariance, ctgInd, ctgVal,
        plotFilt=torch.tensor([0,1]), fig=None, ax=None):
    """Do a 2D scatter plot of a set of responses, and draw ellipses to
    show the 2 SD of the Gaussian distribution.
    Inputs:
    - resp: Tensor with filter responses. (nStim x nFilt)
    - covariance: Tensor with covariances for each class. (nClasses x nFilt x nFilt)
    - ctgInd: Class index of each stimulus. (nStim)
    - ctgVal: Vector with the X values of each category.
            Used for the color code
    - plotFilt: Tensor with the indices of the two filters to plot (i.e. the columns
            of resp, and the sub-covariance matrix of covariance)
    - ax: The axis handle on which to draw the ellipse
    """
    # Get the value corresponding to each data point
    respVal = ctgVal[ctgInd]
    # Select relevant covariances
    # Covariances to plot
    covPlt = subsample_covariance(covariance=covariance,
            classInd=ctgInd.unique(), filtInd=plotFilt)
    # Category values associated with the covariances
    covVal = ctgVal[ctgInd.unique()]
    # Plot responses and ellipses
    if ax is None:
        fig, ax = plt.subplots()
        showPlot = True
    else:
        showPlot = False
    sc = ax.scatter(resp[:, plotFilt[0]], resp[:, plotFilt[1]],
            c=respVal, cmap='viridis', s=5)
    # create ellipses for each covariance matrix
    for i in range(covPlt.shape[0]):
        cov = covPlt[i, :, :]
        # Get ellipse parameters
        scale, eigVec = torch.linalg.eig(cov)
        scale = torch.sqrt(scale).real
        eigVec = eigVec.real
        # Draw Ellipse
        ell = patches.Ellipse(xy=(0,0), width=scale[0]*4, height=scale[1]*4,
                angle=torch.rad2deg(torch.atan2(eigVec[1, 0], eigVec[0, 0])),
                color=sc.to_rgba(covVal[i]))
        ell.set_facecolor('none')
        ell.set_linewidth(2)
        ax.add_artist(ell)
    fig.colorbar(sc)
    plt.xlabel(f'Filter {plotFilt[0]}')
    plt.ylabel(f'Filter {plotFilt[1]}')
    if showPlot:
        plt.show()

